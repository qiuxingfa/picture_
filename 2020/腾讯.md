# 腾讯

### 0. 目录

1. [将整数N消除]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#1-%E5%B0%86%E6%95%B4%E6%95%B0n%E6%B6%88%E9%99%A4](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#1-将整数n消除))
2. [将供需平衡的水果分配]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#2-%E5%B0%86%E4%BE%9B%E9%9C%80%E5%B9%B3%E8%A1%A1%E7%9A%84%E6%B0%B4%E6%9E%9C%E5%88%86%E9%85%8D](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#2-将供需平衡的水果分配))
3. [输出最小值并将剩下的值减最小值，做k次]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#3-%E8%BE%93%E5%87%BA%E6%9C%80%E5%B0%8F%E5%80%BC%E5%B9%B6%E5%B0%86%E5%89%A9%E4%B8%8B%E7%9A%84%E5%80%BC%E5%87%8F%E6%9C%80%E5%B0%8F%E5%80%BC%E5%81%9Ak%E6%AC%A1](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#3-输出最小值并将剩下的值减最小值做k次))
4. [编辑距离]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#4-%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#4-编辑距离))
5. [常用的优化方法]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#5-%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#5-常用的优化方法))
6. [SVM]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#6-svm](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#6-svm))
7. [两个数组取交集]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#7-%E4%B8%A4%E4%B8%AA%E6%95%B0%E7%BB%84%E5%8F%96%E4%BA%A4%E9%9B%86](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#7-两个数组取交集))
8. [过拟合（正则化，Dropout）]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#8-%E8%BF%87%E6%8B%9F%E5%90%88%E6%AD%A3%E5%88%99%E5%8C%96dropout](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#8-过拟合正则化dropout))
9. [Boosting]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#9-boosting](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#9-boosting))
10. [Language Model]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#10-language-model](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#10-language-model))
11. [数据增强]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#11-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#11-数据增强))
12. [排序]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#12-%E6%8E%92%E5%BA%8F](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#12-排序))
13. [查找]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#13-%E6%9F%A5%E6%89%BE](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#13-查找))
14. [RNN，LSTM]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#14-rnnlstm](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#14-rnnlstm))
15. [Transformer]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#15-transformer](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#15-transformer))
16. [调参]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#16-%E8%B0%83%E5%8F%82](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#16-调参))
17. [单链表反转]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#17%E5%8D%95%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#17单链表反转))
18. [判断两个数组是否存在相同的数字]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#18%E5%88%A4%E6%96%AD%E4%B8%A4%E4%B8%AA%E6%95%B0%E7%BB%84%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E7%9B%B8%E5%90%8C%E7%9A%84%E6%95%B0%E5%AD%97](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#18判断两个数组是否存在相同的数字))
19. [Trie树(字典树)的建立和查找]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#19-trie%E6%A0%91%E5%AD%97%E5%85%B8%E6%A0%91%E7%9A%84%E5%BB%BA%E7%AB%8B%E5%92%8C%E6%9F%A5%E6%89%BE](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#19-trie树字典树的建立和查找))
20. [文件40亿+无重复数字，排序到新文件]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#20-%E6%96%87%E4%BB%B640%E4%BA%BF%E6%97%A0%E9%87%8D%E5%A4%8D%E6%95%B0%E5%AD%97%E6%8E%92%E5%BA%8F%E5%88%B0%E6%96%B0%E6%96%87%E4%BB%B6](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#20-文件40亿无重复数字排序到新文件))
21. [python 可变和不可变类型]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#21-python-%E5%8F%AF%E5%8F%98%E5%92%8C%E4%B8%8D%E5%8F%AF%E5%8F%98%E7%B1%BB%E5%9E%8B](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#21-python-可变和不可变类型))
22. [机器学习算法]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#22-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#22-机器学习算法))
23. [CRF]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#23-crf](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#23-crf))
24. [从上到下打印二叉树]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#24-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#24-从上到下打印二叉树))
25. [模拟退火算法]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#25-%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#25-模拟退火算法))
26. [贪婪算法]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#26-%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#26-贪婪算法))
27. [深度优先和广度优先]([https://github.com/qiuxingfa/picture_/blob/master/2020/%E8%85%BE%E8%AE%AF.md#26-%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#26-贪婪算法))







### 1. 将整数N消除

* 当k>0时就把k二等分，不能拆分就返回n，用递归

### 2. 将供需平衡的水果分配

* 每次把前一个村的水果全部给下一个

### 3. 输出最小值并将剩下的值减最小值，做k次

* 先排序后输出

### 4. 编辑距离

* ![](https://img-blog.csdn.net/20181012193113106?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2tvaWJpa2k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
* 动态规划

```python
 def normal_leven(str1, str2):
      len_str1 = len(str1) + 1
      len_str2 = len(str2) + 1
      #create matrix
      matrix = [0 for n in range(len_str1 * len_str2)]
      #init x axis
      for i in range(len_str1):
          matrix[i] = i
      #init y axis
      for j in range(0, len(matrix), len_str1):
          if j % len_str1 == 0:
              matrix[j] = j // len_str1
          
      for i in range(1, len_str1):
          for j in range(1, len_str2):
              if str1[i-1] == str2[j-1]:
                  cost = 0
              else:
                  cost = 1
              matrix[j*len_str1+i] = min(matrix[(j-1)*len_str1+i]+1,
                                          matrix[j*len_str1+(i-1)]+1,
                                          matrix[(j-1)*len_str1+(i-1)] + cost)
          
      return matrix[-1]
```

### 5. 常用的优化方法

* BGD(Batch Gradient Decent)

  * 采用整个训练集的数据来计算cost function对参数的梯度
  * 缺点：计算起来非常慢，不能投入新数据实时更新模型

* SGD(Stochastic Grandient Descent)

  * 缺点：SGD因为更新比较频繁，会造成cost function有严重的震荡

* Mini-Batch Gradient Descent(MBGD)

  * 每次用一批数据进行计算
  * 缺点：不能保证很好的收敛性

* Momentum

  * 加速SGD，并且抑制震荡（惯性）

* Nesterov Accelerated Gradient

  * 在未来的位置上计算梯度

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310223504871-1752782669.png)

  * 避免走得太快

* Adagrad(Adaptive gradient algrorithm)

  * 可以对低频参数做较大更新，对高频参数做较小更新，对于稀疏数据表现很好，很好地提高了SGD的鲁棒性

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310225616227-189502936.png)

  * 优点：不需要对学习率进行手动调节
  * 缺点：分母会不断累积，学习率最终会变得非常小

* Adadelta

  * 对adagrad的优化，将分母G换成了过去梯度平方的衰减平均值

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311101514058-575958058.png)

* RMSprop

  * 自适应学习率

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311102340145-62793612.png)

  * 使用的是指数加权平均，旨在消除梯度下降中的摆动

* Adam

  * RMSprop+Momentum
  * 既存储了过去梯度平方的指数衰减平均，也保持了过去梯度的指数衰减平均，并且做了偏差修正

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105115382-532376237.png)

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105330568-34539188.png)

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105355059-406281512.png)

* 如何选择

  * 如果数据是稀疏的，就用自适应方法，即Adagrad、Adadelta、RMSprop、Adam
  * SGD虽然能达到极小值，但是比其他算法用的时间长，而且可能会北困在鞍点

### 6. SVM

* 是一种二分类模型，是定义在特征空间上的间隔最大的线性分类器，基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面（任意点到平面的距离要超过γ）
* 最大化γ等价于最小化1/2 ||w||^2
* 对于含有不等式约束的凸二次规划问题，对其使用拉格朗日乘子法得到其对偶问题
* 将有约束的原始目标函数转为无约束的新构造的拉格朗日目标函数

![](https://www.zhihu.com/equation?tex=L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3D%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%5Cleft%28+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1+%5Cright%29%7D+)

![](https://www.zhihu.com/equation?tex=+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cunderset%7B%5Calpha+_%7B_i%7D%5Cge+0%7D%7B%5Cmax%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+)

* 当样本点在可行解区域之外时，将α设为无穷大，样本点满足约束时，即为原函数

![](https://www.zhihu.com/equation?tex=+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2%5C+%2C%5Cboldsymbol%7Bx%7D%5Cin+%5Ctext%7B%E5%8F%AF%E8%A1%8C%E5%8C%BA%E5%9F%9F%7D%5C%5C+%2B%5Cinfty+%5C+%5C+%5C+%5C+%5C+%2C%5Cboldsymbol%7Bx%7D%5Cin+%5Ctext%7B%E4%B8%8D%E5%8F%AF%E8%A1%8C%E5%8C%BA%E5%9F%9F%7D%5C%5C+%5Cend%7Bcases%7D+)

* 原约束问题就等价于

![](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5Cunderset%7B%5Calpha+_i%5Cge+0%7D%7B%5Cmax%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3Dp%5E%2A+)

* 使用拉格朗日函数的对偶性，先求最小，再求最大，需满足条件

  * 优化问题时凸优化问题
  * 满足KKT条件(保证极值点的一些约束条件，使一组解为最优解的必要条件)

  ![](https://www.zhihu.com/equation?tex=+%5Cbegin%7Bcases%7D+%5Calpha+_i%5Cge+0%5C%5C+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1%5Cge+0%5C%5C+%5Calpha+_i%5Cleft%28+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1+%5Cright%29+%3D0%5C%5C+%5Cend%7Bcases%7D+)

* 令L对w和b求偏导为0，求得参数带入L，消去L和b

* ![](https://www.zhihu.com/equation?tex=%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3D-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_j%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D%2B%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+)

* 最后转化为求解极小

![](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7B%5Calpha+%7D%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_j%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+)

* 使用 序列最小优化（SMO）算法求解
* 后求出w和b
* 训练完成后，大部分样本不需要保留，最终模型只与支持向量相关



* 对于线性不可分的情况，引入软间隔（加入松弛变量），即允许某些点不满足约束，采用hinge loss

![](https://www.zhihu.com/equation?tex=%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%2C%5Cxi+_i%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2%2BC%5Csum_%7Bi%3D1%7D%5Em%7B%5Cxi+_i%7D+)

![](https://www.zhihu.com/equation?tex=%5Cxi+_i%3D%5Cmax+%5Cleft%28+0%2C1-y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cright%29+)



* 算法流程

  * 选取适当的核函数，和惩罚函数，构造并求解凸二次规划问题，得到最优解

  ![](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7B%5Calpha+%7D%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_jK%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2C%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+)

  * 计算w和b
  * 求分离超平面
  * 分类决策函数为

  ![](https://www.zhihu.com/equation?tex=+f%5Cleft%28+x+%5Cright%29+%3Dsign%5Cleft%28+%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_%7Bi%7D%5E%7B%2A%7Dy_iK%5Cleft%28+x%2Cx_i+%5Cright%29+%2Bb%5E%2A%7D+%5Cright%29+)

  * 常用核函数(高斯函数)

  ![](https://www.zhihu.com/equation?tex=+K%5Cleft%28+x%2Cz+%5Cright%29+%3D%5Cexp+%5Cleft%28+-%5Cfrac%7B%5ClVert+x-z+%5CrVert+%5E2%7D%7B2%5Csigma+%5E2%7D+%5Cright%29+)

  

### 7. 两个数组取交集

* 排序，双指针

### 8. 过拟合（正则化，Dropout）

* 欠拟合：模型在训练集上表现差，没有学习到数据的规律，原因：模型过于简单（数据是非线性的使用了线性模型），特征数太少
* 过拟合：模型在训练集上表现好，在测试集上表现不好，泛化性能差
* 原因：
  * 模型本身过于复杂，需选用更简单的模型或者对模型进行裁剪
  * 训练样本太少或者缺乏代表性，增加样本数或者增加样本的多样性
  * 训练样本噪声的干扰，需剔除噪声数据或改用对噪声不敏感的数据
* 正则化
  * 在损失函数上增加一个惩罚项，防止系数过大让模型变得复杂
  * L1正则化更适用于特征选择，每次更新都会加上一个常数，所以很容易产生的特征的系数为0的情况，会让特征变得稀疏
  * L2正则化更适用于防止模型过拟合，每次更新会对特征稀疏进行一个比例的缩放，这会让系数趋向变小而不会变为0，会让模型变得简单，防止过拟合
* 剪枝
* 数据增广
  * 对图像进行旋转、缩放、剪切、添加噪声等
  * 做同义词替换扩充数据集
  * 对样本数据添加随机噪声
* Dropout
  * 在训练时随机选择一部分神经元进行正向和反向传播，另外一些神经元参数值保持不变，每个神经元只使用了部分样本进行训练，最终得到了多个神经网络的组合
  * 通过降低节点之间的关联性以及模型的复杂度，从而达到正则化的效果
* Early Stopping
  * 在验证集误差出现增大后，提前结束训练
* 集成学习
  * 通过多个模型的结果，来降低模型的方差
* bagging
  * 从原始样本集中抽取训练集，每轮从样本集中使用Bootstrapping的方法抽取n个训练样本，共进行k轮抽取，得到k个训练集
  * 得到k个模型，投票得到最终结果

* boosting
  * 每轮训练时减小上一轮训练正确的样本权重，增大错误的样本权重，下一轮的训练的目标是找到一个函数f来拟合上一轮的残差



### 9. Boosting

* 随机森林

  * 通过集成学习将多棵树集成的算法
  * 依靠决策树的投票选择来决定最后的分类结果
  * 对数据集进行有放回的随机抽样，可以降低模型的方差，如果不放回地抽样，每棵树用的样本完全不同，基学习器之间的相似性小，投票结果差，模型偏差大。引入了噪音。

* Bootstrap抽样

  * 在全部样本未知的情况下，借助部分样本的又放回的多次抽样

* Adaboost

  * 开始时，所有的样本权重相同，从第二轮开始，提高上一轮分错样本的权重，降低上一轮分对样本的权重，边界附近的样本点会得到更多的重视
  * 将各个弱分类器组成强分类器，分类误差小的弱分类器话语权大

* XGboost

  * 是一个树集成模型，将k个树的结果进行求和，得到最终的预测值
  * 训练时，新的一轮加入一个新的函数，来最大化的降低目标函数

  ![](https://upload-images.jianshu.io/upload_images/5012681-7d96b3eaab4209af.png?imageMogr2/auto-orient/strip|imageView2/2/w/926/format/webp)

  * 将目标函数进行泰勒展开，取前三项
  * 缺失数据会被分到左子树和右子树分别计算损失，选择较优的那一个

* GBDT

  * 梯度下降树，通过不断减小训练过程产生的残差来达到将数据分类或者回归的算法
  * 弱分类器一般选择CART TREE（分类回归树）
  * 每轮迭代的时候，都去拟合损失函数在当前模型下的负梯度
  * CART TREE的生成过程实际上就是特征选择的过程，遍历每个特征，然后对每个特征遍历它所有可能的切分点，找到最优特征m的最优切分点j，最优即loss最小

  ![](https://images2017.cnblogs.com/blog/666027/201710/666027-20171031134114980-1949308807.png)

  * 如何用于分类

  



* XGBoost和GBDT的对比
  * GBDT的基分类器只支持CART树，而XGBoost支持线性分类器
  * GBDT在优化时只使用了一阶倒数，而XGBoost对目标函数进行了二阶泰勒展开，此外XGBoost支持自定义损失函数，只要损失函数二阶可导

### 10. Language Model

* 定义：语言模型是对语句的概率分布的建模
* n-gram语言模型

![](https://www.zhihu.com/equation?tex=P%28w_1%2Cw_2%2C...%2Cw_n%29%3DP%28w_1%29P%28w_2%7Cw_1%29%5Ccdot%5Ccdot%5Ccdot+P%28w_n%7Cw_1%2C...%2Cw_%7Bn-1%7D%29)

* 马尔可夫假设，即当前词出现的概率只依赖于前n-1个词

![](https://www.zhihu.com/equation?tex=P%28w_i%7Cw_1%2Cw_2%2C...%2Cw_%7Bi-1%7D%29%3DP%28w_i%7Cw_%7Bi-n%2B1%7D%2C...%2Cw_%7Bi-1%7D%29)

* 第一代PTM通常采用浅层模型，尽管它们也可以捕捉单词的语义，但它们却不受上下文的限制，只是简单地学习共现词频，这样的方法明显无法理解更高层的文本概念，而第二代PTM专注于学习上下文的词嵌入，

* ELMO

  ![](https://img2018.cnblogs.com/blog/670089/201810/670089-20181021105842585-294009665.png)

  * 可得到上下文依赖的当前词表示

* GPT

  * 使用了transformer，在大数据集上使用语言模型作为附加任务效果更好，并且transformer比LSTM效果更好
  * 单向模型

* BERT

  ![](https://img2018.cnblogs.com/blog/670089/201810/670089-20181021105847524-1030716839.png)

  * MLM：随机挡上15%的词，进行预测
  * 预测下一个就句子
  * 双向特征的自编码预训练语言模型

* XLNET

  * BERT忽略了被mask位置之间的依赖关系，因此出现预训练和微调效果的差异
  * 自回归（单向），根据上文内容预测下一个可能跟随的单词根据上文内容预测下一个可能跟随的单词
    * 缺点是不能同时利用上文和下文的信息，ELMO的融合模式过于简单
    * 优点在于对于生成式的任务天然匹配
  * 通过对句子中的单词排列组合，把一部分Ti下文的单词排到Ti上文的位置
  * 对长文本类型的NLP应用占优势



### 11. 数据增强

* 加噪
  * 随机扔词，如随机在Embedding上加dropout
* Back Translation，反向翻译
* 同义词替换、插入、交换和删除（同义词在NLP中通常具有比较相近的词向量，因此对于模型来说，并没有起到比较好的对数据增强的作用）
* 句法、句子扩充、句子缩写
* 随机打乱文本词序

### 12. 排序

* ![](https://images2015.cnblogs.com/blog/975503/201702/975503-20170214211234550-1109833343.png)
* 冒泡排序
  * 重复走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来
  * 时间复杂度O(n^2)，稳定
* 快速排序
  * 从数列中跳出一个元素，称为“基准”,所有元素比基准小的摆在基准前面，比基准值大的摆在基准值后面，进行递归
  * 时间复杂度O(nlogn)，不稳定
* 直接插入排序
  * 将数据插入到已经排好序的有序数据中
  * 时间复杂度为O(n^2)，稳定
* 希尔排序
  * 按下表的一定增量分组，对每组使用直接插入排序算法排序
  * 时间复杂度为O(n^1.3)，不稳定
* 直接选择排序
  * 在序列中找到最小元素，存放到排序序列的起始位置，然后再从剩余序列中寻找最大元素，放到排序序列的末尾
* 堆排序
  * 大根堆调整，建立大根堆，堆排序
* 归并排序
  * 将n个元素平均划分为n/2个元素的子序列，递归解决两个规模为n/2的子问题，合并两个已排序的子序列
* 基数排序
  * 按照个位数将数值分配至0-9编号的桶里，再按十位数分配到编号为0-9的桶里



### 13. 查找

* 顺序查找
  * 按照序列原有顺序对数组进行遍历比较查询
* 折半查找
  * 输入有序
  * 每次选取中间值，如果小于中间值，则再比较左边元素，否则比较右边元素
* 分块查找
  * 将n个数据分为m块，块内无序，按块有序
  * 选取各块中的最大关键字构成索引表，对索引表进行二分查找或顺序查找，以缺点待查记录在哪一块中，在已确定 的块中用顺序法进行查找



### 14. RNN，LSTM

* RNN
  * 优点：可以拟合序列数据
  * 在长文本的情况下，学习不到之前的信息
* LSTM
  * 通过遗忘门和输出门忘记部分信息来解决梯度消失问题，导数为累加形式，避免了梯度消失，通过设置阈值和使用正则化来解决梯度爆炸问题
  * 信息在过远的距离传播中损失很厉害
  * 无法很好地并行
* GRU
  * 参数少，收敛快，通常在数据集够大的情况下，选择LSTM效果应该会更好
  * 通常情况下两者效果相差不大，GRU训练更快

### 15. Transformer

* attention

![](https://img-blog.csdn.net/2018082211021619?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhaGFqaW5idQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* self-attention

  ![](https://pic2.zhimg.com/80/v2-32eb6aa9e23b79784ed1ca22d3f9abf9_720w.jpg)

  ![](https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29+%3D+softmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V)

  * self-attention是attention的一种特殊形式,自注意力模型其实就是kqv是相等的,在序列内部做attention,一个句子的每个词都要对该句的所有词进行attention计算,目的是学习句子内部的依赖关系,捕获句子的内部结构

  * 在encoder-decoder连接处,kv来自encoder,q来自decoder

* multihead就是有不同的QKV表示,最后将其结果结合起来

* add & norm:残差和归一化

* NLG(自然语言生成)

  * 基于模板,基于树

### 16. 调参

* 泛化误差,模型在测试集表现不好,过拟合和欠拟合都会让泛化误差高
* 激活函数的选择
  * 对于输出层,多分类任务选用softmax,二分类选择sigmoid,回归任务选择线性输出
  * 对于中间隐层,优先选择relu(可以有效解决sigmoid和tanh出现的梯度弥散问题)
  * RNN要优先选用tanh激活函数
* 学习率设定
  * 一般从0.1或0.01开始
  * 太大会导致训练不稳定,甚至出现Nan,太小会导致损失下降太慢
* 防止过拟合
  * L1,L2正则化,dropout,提前终止和数据集扩充
* 优化器选择
  * 如果数据稀疏,就用自适应方法,即Adagrad,Adadelta,RMSprop,Adam
  * SGD虽然能达到最小值,但是比其他算法用的时间更长,而且可能会被困在鞍点
* 残差块和BN层
  * 残差块可以让网络更深
  * BN层可以加速训练速度,有效防止梯度消失与爆炸,具有防止过拟合的效果
* 自动调参方法
  * Grid Search,网格搜索,在所有候选的参数选择中,通过循环遍历,尝试每一种可能
  * Random Search
  * Bayesian Optimization:贝叶斯优化,考虑到了不同参数对应的实验结果,迭代次数更少,速度更快
* 参数随机初始化与数据预处理
* 提升召回率
  * 增加数据量

### 17.单链表反转

* 就地逆序：定义前一节点,当前节点,后一节点三个指针
* 插入法逆序：从链表的第二个节点开始，把遍历到的节点插入到头节点的后面，直到遍历结束（不需要额外的变量）
* 递归：如果第一个节点后面的n-1个节点已经正确反转的话，只要处理第一第二个节点的指向关系就行了

~~~python
def reverse2(head):
        if head.next == None:  # 递归停止的基线条件
            return head
        new_head = reverse2(head.next)
        head.next.next = head	# 当前层函数的head节点的后续节点指向当前head节点
        head.next = None	# 当前head节点指向None
        return new_head
~~~



### 18.判断两个数组是否存在相同的数字

* 遍历其中一个数组,在第二个数组使用二分查找
* 设置两个指针,顺序比较

### 19. Trie树(字典树)的建立和查找

* trie是一颗存储多个字符串的树,相邻节点的边代表一个字符,树的叶节点则代表完整的字符串

![](http://blog.chinaunix.net/attachment/201307/18/28977986_1374115884WXdK.jpg)

* 构建trie的基本算法为:逐一把每则单词的每个字母插入trie,如果存在就共享,不存在就构建对应的边和权重

### 20. 文件40亿+无重复数字，排序到新文件

* 使用位图bitmap,一个bit存储一个数字,那么40亿数据约为40亿bit约为500M内存
* 表示为32位的二进制,然后进行查找

### 21. python 可变和不可变类型

* 指内存中的那块内容（value）是否可以被改变
* 可变:列表、字典，不需要再在其他地方申请内存
* 不可变：数字，字符串，元组，不可变类型在对对象本身操作的时候，必须在内存中申请新一块区域

### 22. 机器学习算法

* 决策树

  * 非参数学习算法，用于解决分类问题和回归问题
  * 每个内部节点表示一个属性上的判断，每个分支代表判断结果的输出，最后每个叶节点代表一种分类结果
  * 比较常用的决策树有ID3（用熵增原理来决定哪个做父节点，哪个节点需要分裂），C4.5（ID3分割太细可能过拟合，优化项要除以分割太细的代价，这个比值叫做信息增益率）和CART（分类回归树，选择GINI指数最小方案，总体内包含的类别越杂乱）

* 朴素贝叶斯分类器

  * 假设各个特征之间相互独立

  ![](https://pic1.zhimg.com/80/v2-a2a73f43adcbb0bf4b9bae19b9495f81_720w.jpg)


  * 先验概率p(类别)，p(特征)
  * 后验概率：事情已经发生，要求这件事情发生的原因是某个因素引起的可能性大小

* 最小二乘法

  * 线性回归

  ![](https://www.zhihu.com/equation?tex=%0A%5Cbegin%7Bcases%7D%0A%20%20%20%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20a%7D%5Cepsilon%3D2%5Csum%20(ax_i%2Bb-y_i)x_i%3D0%5C%5C%0A%20%20%20%20%5Cquad%5C%5C%0A%20%20%20%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20b%7D%5Cepsilon%3D2%5Csum%20(ax_i%2Bb-y_i)%3D0%0A%5Cend%7Bcases%7D%0A)

* 逻辑回归

  * 使用sigmoid函数作为判别函数
  * 使用交叉熵作为损失函数

  ![](https://img-blog.csdnimg.cn/20181110222213701.jpg)

  * 缺点：容易欠拟合，分类精度不高；数据特征有缺失或者特征空间很大时表现 效果并不好

* SVM

  * 是一种二分类模型，是定义在特征空间上的间隔最大的线性分类器，基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面（任意点到平面的距离要超过γ）

* LR和SVM

  * LR采用交叉熵损失（log损失），SVM使用合页损失
  * LR对异常值敏感，SVM对异常值不敏感
  * 训练集较小时，SVM比较适用，而LR需要较多的样本
  * LR尽量让所有点都远离超平面，而SVM只是让靠近中间分割线的那些点尽量远离

* 集成方法

  * bagging
  * boosting

* 聚类算法

  * 基于划分
  * 基于密度
  * 基于网格
  * 凝聚层次聚类
  * 谱聚类

* 主成分分析（PCA）

  * 数据降维，将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分
  * 选择最大差异性的主成分方向

* 奇异值分解（SVD）
  $$
  A = UΣV^T
  $$

  * U和V均为单位正交矩阵

* 独立成分分析（ICA）

  * X = As

### 23. CRF

* 序列化标注算法

* HMM

  * 能够计算出给定一个词和它可能的词性的联合概率分布

  ![](http://images0.cnblogs.com/blog/133059/201507/161450321576527.png)

  * 

  * HMM假设了两类特征，一是当前词词性与上一个词词性的关系（转移概率），以及当前词语和当前词性的关系（发射概率），HMM的训练过程就是在训练集中学习这两个概率矩阵

* CRF可以看成是序列化的logistic regression

![](https://pic2.zhimg.com/80/1d3f9cefc0de33cfebe71bbc237ccc6b_720w.jpg)

* 特征函数
  * 句子s
  * 句子s中的第i个单词
  * 要评分的标注序列给第i个单词标注的词性
  * 要评分的标注序列给第i-1个单词标注的词性
* 判别式模型是直接对P（Y|X）建模，生成式模型是对P（X，Y）建模
* Viterbe解码，最优路径搜索算法
  * 从开始状态后每走一步，就记录下到达该状态的所有路径的概率最大值，然后以此最大值为基准继续向后推进，显然，如果这个最大值都不能使该状态成为最大似然状态路径上的结点的话，那些小于它的概率值就更没有可能了



### 24. 从上到下打印二叉树

* 保存当前层还没有打印的节点和下一层的节点
* 之字形打印：打印某一层节点时，把下一层的子节点保存到相应的栈里，如果当前为奇数层，则先保存左子节点，再保存右子节点，如果当前层为偶数层，则先保存右子节点再保存左子节点

### 25. 模拟退火算法

* 模拟退火会以一定概率跳出局部最优解

![](https://upload-images.jianshu.io/upload_images/5501600-829414a6d94e3565.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

### 26. 贪婪算法

* 在面对问题求解时，总是选择做出在当前看起来是最好的选择，不从整体最优上加以考虑，它所做出的仅仅是在某种意义上的局部最优解

### 27. 深度优先和广度优先

