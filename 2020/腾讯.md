# 腾讯

### 0. 目录

1. [将整数N消除](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#1-将整数n消除)
2. [将供需平衡的水果分配](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#2-将供需平衡的水果分配)
3. [输出最小值并将剩下的值减最小值，做k次](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#3-输出最小值并将剩下的值减最小值做k次)
4. [编辑距离](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#4-编辑距离)
5. [常用的优化方法](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#5-常用的优化方法)
6. [SVM](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#6-svm)
7. [两个数组取交集](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#7-两个数组取交集)
8. [过拟合（正则化，Dropout）](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#8-过拟合正则化dropout)
9. [Boosting](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#9-boosting)
10. [Language Model](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#10-language-model)
11. [数据增强](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#11-数据增强)
12. [排序](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#12-排序)
13. [查找](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#13-查找)
14. [RNN，LSTM](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#14-rnnlstm)
15. [Transformer](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#15-transformer)
16. [调参](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#16-调参)
17. [单链表反转](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#17单链表反转)
18. [判断两个数组是否存在相同的数字](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#18判断两个数组是否存在相同的数字)
19. [Trie树(字典树)的建立和查找](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#19-trie树字典树的建立和查找)
20. [文件40亿+无重复数字，排序到新文件](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#20-文件40亿无重复数字排序到新文件)
21. [python 可变和不可变类型](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#21-python-可变和不可变类型)
22. [机器学习算法](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#22-机器学习算法)
23. [CRF](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#23-crf)
24. [从上到下打印二叉树](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#24-从上到下打印二叉树)
25. [模拟退火算法](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#25-模拟退火算法)
26. [贪婪算法](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#26-贪婪算法)
27. [深度优先和广度优先](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#26-贪婪算法)
28. [迭代器与生成器，装饰器](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#28-迭代器与生成器装饰器)
29. [pytorch和tensorflow的区别](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#29-pytorch和tensorflow的区别)
30. [word2vec](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#30-word2vec)
31. [最小的k个数](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#31-最小的k个数)
32. [剪绳子](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#33-剪绳子)
33. [评估指标](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#33-评估指标)
34. [KMP算法](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#34-kmp算法)
35. [降维](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#35-降维)
36. [知识蒸馏](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#36-知识蒸馏)
37. [Linux](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#37-linux)
38. [并查集](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#38-并查集)
39. [树的遍历](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#39-树的遍历)
40. [幂集](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#40-幂集)
41. [数组中未出现的最小正整数](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#41-数组中未出现的最小正整数)
42. [连续子数组的最大和](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#42-连续子数组的最大和)
43. [项目自述](https://github.com/qiuxingfa/picture_/blob/master/2020/腾讯.md#项目自述)





### 1. 将整数N消除

* 当k>0时就把k二等分，不能拆分就返回n，用递归

~~~python
n_k = list(map(int, sys.stdin.readline().strip().split()))
    n, k = n_k[0], n_k[1]
    count = 0
    for _ in range(k):
        if n <= 2:
            break
        if n%2 == 1:
        	n = (n // 2) + 1 
        else:
            n // 2
        count += 1
    print(count + n)
~~~





### 2. 将供需平衡的水果分配

* 每次把前一个村的水果全部给下一个

### 3. 输出最小值并将剩下的值减最小值，做k次

* 先排序后输出

### 4. 编辑距离

* ![](https://img-blog.csdn.net/20181012193113106?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2tvaWJpa2k=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
* 动态规划

```python
 def edit(a, b):
    n1 = len(a)
    n2 = len(b)
    dp = []
    for i in range(n1+1):
        dp.append([0]*(n2+1))
    
    for i in range(n1+1):
        dp[i][0] = i
    for j in range(n2+1):
        dp[0][j] = j
        
    for i in range(1,n1+1):
        for j in range(1, n2+1):
            if a[i-1]==b[j-1]:
                v = 0
            else:
                v = 1
            dp[i][j] = min(dp[i-1][j-1]+v,dp[i-1][j]+1,dp[i][j-1]+1)
    return dp[-1][-1]
```

### 5. 常用的优化方法

* BGD(Batch Gradient Decent)

  * 采用整个训练集的数据来计算cost function对参数的梯度
  * 缺点：计算起来非常慢，不能投入新数据实时更新模型

* SGD(Stochastic Grandient Descent)

  * 以当前位置的负梯度方向为搜索方向，通过每个样本来迭代更新
  * 缺点：SGD因为更新比较频繁，噪音较多，会造成cost function有严重的震荡

* Mini-Batch Gradient Descent(MBGD)

  * 每次用一批数据进行计算
  * 缺点：不能保证很好的收敛性

* Momentum

  * 加速SGD，并且抑制震荡（惯性）,y一般取0.9左右

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310222959486-335611193.png)

* Nesterov Accelerated Gradient

  * 在未来的位置上计算梯度

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310223504871-1752782669.png)

  * 避免走得太快

* Adagrad(Adaptive gradient algrorithm)

  * 可以对低频参数做较大更新，对高频参数做较小更新，对于稀疏数据表现很好，很好地提高了SGD的鲁棒性

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180310225616227-189502936.png)

  * 优点：不需要对学习率进行手动调节
  * 缺点：分母会不断累积，学习率最终会变得非常小

* Adadelta

  * 对adagrad的优化，将分母G换成了过去梯度平方的衰减平均值

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311101514058-575958058.png)

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311102007929-961929459.png)

  

* RMSprop

  * 自适应学习率

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311102340145-62793612.png)

  * 使用的是指数加权平均，旨在消除梯度下降中的摆动(**与Momentum的效果一样**)

* Adam

  * RMSprop+Momentum
  * 既存储了过去梯度平方的指数衰减平均，也保持了过去梯度的指数衰减平均，并且做了偏差修正

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105115382-532376237.png)

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105330568-34539188.png)

  ![](https://images2018.cnblogs.com/blog/1192699/201803/1192699-20180311105355059-406281512.png)

* 如何选择

  * 如果数据是稀疏的，就用自适应方法，即Adagrad、Adadelta、RMSprop、Adam
  * SGD虽然能达到极小值，但是比其他算法用的时间长，而且可能会北困在鞍点

* 牛顿法（二阶收敛）

  * 高维的情况下，计算和存储都很大
  * 目标函数非凸的适合，容易手打鞍点或极大值点的吸引

### 6. SVM

* 是一种二分类模型，是定义在特征空间上的间隔最大的线性分类器，基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面（任意点到平面的距离要超过γ）
* 最大化γ等价于最小化1/2 ||w||^2
* 对于含有不等式约束的凸二次规划问题，对其使用拉格朗日乘子法得到其对偶问题
* 将有约束的原始目标函数转为无约束的新构造的拉格朗日目标函数

![](https://www.zhihu.com/equation?tex=L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3D%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%5Cleft%28+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1+%5Cright%29%7D+)

![](https://www.zhihu.com/equation?tex=+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cunderset%7B%5Calpha+_%7B_i%7D%5Cge+0%7D%7B%5Cmax%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+)

* 当样本点在可行解区域之外时，将α设为无穷大，样本点满足约束时，即为原函数

![](https://www.zhihu.com/equation?tex=+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2%5C+%2C%5Cboldsymbol%7Bx%7D%5Cin+%5Ctext%7B%E5%8F%AF%E8%A1%8C%E5%8C%BA%E5%9F%9F%7D%5C%5C+%2B%5Cinfty+%5C+%5C+%5C+%5C+%5C+%2C%5Cboldsymbol%7Bx%7D%5Cin+%5Ctext%7B%E4%B8%8D%E5%8F%AF%E8%A1%8C%E5%8C%BA%E5%9F%9F%7D%5C%5C+%5Cend%7Bcases%7D+)

* 原约束问题就等价于

![](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+%5Ctheta+%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5Cunderset%7B%5Calpha+_i%5Cge+0%7D%7B%5Cmax%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3Dp%5E%2A+)

* 使用拉格朗日函数的对偶性，先求最小，再求最大，需满足条件

  * 对偶问题将原始问题中的约束转为了对偶问题中的等式约束，方便核函数的引入
  * 优化问题时凸优化问题
  * 满足KKT条件（原问题与对偶问题等价的必要条件，当原问题是凸优化问题时，变为充要条件）(保证极值点的一些约束条件，使一组解为最优解的必要条件)

  ![](https://www.zhihu.com/equation?tex=+%5Cbegin%7Bcases%7D+%5Calpha+_i%5Cge+0%5C%5C+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1%5Cge+0%5C%5C+%5Calpha+_i%5Cleft%28+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+-1+%5Cright%29+%3D0%5C%5C+%5Cend%7Bcases%7D+)

* 令L对w和b求偏导为0，求得参数带入L，消去L和b

* ![](https://www.zhihu.com/equation?tex=%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+L%5Cleft%28+%5Cboldsymbol%7Bw%2C%7Db%2C%5Cboldsymbol%7B%5Calpha+%7D+%5Cright%29+%3D-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_j%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D%2B%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+)

* 最后转化为求解极小

![](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7B%5Calpha+%7D%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_j%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+)

* 使用 序列最小优化（SMO）算法求解
* 后求出w和b
* 训练完成后，大部分样本不需要保留，最终模型只与支持向量相关



* 对于线性不可分的情况，引入软间隔（加入松弛变量），即允许某些点不满足约束，采用hinge loss

![](https://www.zhihu.com/equation?tex=%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%2C%5Cxi+_i%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2%2BC%5Csum_%7Bi%3D1%7D%5Em%7B%5Cxi+_i%7D+)

![](https://www.zhihu.com/equation?tex=%5Cxi+_i%3D%5Cmax+%5Cleft%28+0%2C1-y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cright%29+)

* 损失函数

![](https://img-blog.csdnimg.cn/20190714133329657.png)

* 算法流程

  * 选取适当的核函数，和惩罚函数，构造并求解凸二次规划问题，得到最优解

  ![](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7B%5Calpha+%7D%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Csum_%7Bj%3D1%7D%5EN%7B%5Calpha+_i%5Calpha+_jy_iy_jK%5Cleft%28+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2C%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bj%7D%7D+%5Cright%29%7D%7D-%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_i%7D+)

  * 计算w和b
  * 求分离超平面
  * 分类决策函数为

  ![](https://www.zhihu.com/equation?tex=+f%5Cleft%28+x+%5Cright%29+%3Dsign%5Cleft%28+%5Csum_%7Bi%3D1%7D%5EN%7B%5Calpha+_%7Bi%7D%5E%7B%2A%7Dy_iK%5Cleft%28+x%2Cx_i+%5Cright%29+%2Bb%5E%2A%7D+%5Cright%29+)

  * 常用核函数

    * 高斯核

      ![](https://www.zhihu.com/equation?tex=+K%5Cleft%28+x%2Cz+%5Cright%29+%3D%5Cexp+%5Cleft%28+-%5Cfrac%7B%5ClVert+x-z+%5CrVert+%5E2%7D%7B2%5Csigma+%5E2%7D+%5Cright%29+)

    * 线性核

    ![](https://img-blog.csdn.net/20140630140445046)

    * 多项式核

    ![](https://img-blog.csdn.net/20140707130435312)

    * 幂指数核

    ![](https://img-blog.csdn.net/20140630143612390)

  

### 7. 两个数组取交集

* 排序，双指针

### 8. 过拟合（正则化，Dropout）

* 欠拟合：模型在训练集上表现差，没有学习到数据的规律，原因：模型过于简单（数据是非线性的使用了线性模型），特征数太少

* 过拟合：模型在训练集上表现好，在测试集上表现不好，泛化性能差

* 原因：

  * 模型本身过于复杂，需选用更简单的模型或者对模型进行裁剪
  * 训练样本太少或者缺乏代表性，增加样本数或者增加样本的多样性
  * 训练样本噪声的干扰，需剔除噪声数据或改用对噪声不敏感的数据

* 正则化

  * 在损失函数上增加一个惩罚项，防止系数过大让模型变得复杂，减少过拟合

  ![](https://www.zhihu.com/equation?tex=l_%7B1%7D%3A%5COmega%5Cleft%28+w+%5Cright%29%3D%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B1%7D%3D%5Csum_%7Bi%7D%7B%5Cleft%7C+w_%7Bi%7D+%5Cright%7C%7D)

  ![](https://www.zhihu.com/equation?tex=l_%7B2%7D%3A%5COmega%5Cleft%28+w+%5Cright%29%3D%5Cleft%7C+%5Cleft%7C+w+%5Cright%7C%5Cright%7C_%7B2%7D%5E%7B2%7D%3D%5Csum_%7Bi%7D%7Bw_%7Bi%7D%5E%7B2%7D%7D)

  * L1正则化更适用于特征选择，每次更新都会加上一个常数，所以很容易产生的特征的系数为0的情况，会让特征变得稀疏
  * L2正则化更适用于防止模型过拟合，每次更新会对特征稀疏进行一个比例的缩放，这会让系数趋向变小而不会变为0，会让模型变得简单，防止过拟合
  * 决策树剪枝
  * BN

  ![](https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142956288-903484055.png)

  * ![](https://img2018.cnblogs.com/blog/1470684/201908/1470684-20190812210509089-346979698.png)
  * LN针对单个样本进行，不依赖于其他数据，使用于RNN
  * BN一定程度上增加了泛化能力，减轻了对参数初始化的依赖， 训练更快，可以使用更高的学习率。

* 剪枝

* 数据增广

  * 对图像进行旋转、缩放、剪切、添加噪声等
  * 做同义词替换扩充数据集
  * 对样本数据添加随机噪声

* Dropout

  * 在训练时随机选择一部分神经元进行正向和反向传播，另外一些神经元参数值保持不变，每个神经元只使用了部分样本进行训练，最终得到了多个神经网络的组合
  * 通过降低节点之间的关联性以及模型的复杂度，从而达到正则化的效果

* Early Stopping

  * 在验证集误差出现增大后，提前结束训练



* 集成学习

  * 通过多个模型的结果，来降低模型的方差

* bagging
  * 从原始样本集中抽取训练集，每轮从样本集中使用Bootstrapping的方法抽取n个训练样本，共进行k轮抽取，得到k个训练集
  * 得到k个模型，投票得到最终结果

* boosting

  * 每轮训练时减小上一轮训练正确的样本权重，增大错误的样本权重，下一轮的训练的目标是找到一个函数f来拟合上一轮的残差



### 9. Boosting

* 随机森林(bagging)

  * 通过集成学习将多棵树集成的算法
  * 随机选择样本（放回抽样）；**随机选择特征属性**；构建决策树；随机森林投票（平均） 因此防止过拟合能力更强，降低方差。
  * 依靠决策树的投票选择来决定最后的分类结果
  * bootstrap：对数据集进行有放回的随机抽样，可以降低模型的方差，如果不放回地抽样，每棵树用的样本完全不同，基学习器之间的相似性小，投票结果差，模型偏差大。引入了噪音。
  * 对高维数据集的处理能力很好，可以处理成千上万的输入变量，并且确定最重要的变量，因此被认为是不错的降维方法。高度并行化，易于分布式实现

* bagging减少方差，boosting减少偏差

* Bootstrap抽样

  * 在全部样本未知的情况下，借助部分样本的又放回的多次抽样
  * Bagging采用有放回的均匀取样，而Boosting根据错误率来取样

* Adaboost（损失函数为指数损失）

  * 算法流程：

  ![](C:\Users\qxf\AppData\Roaming\Typora\typora-user-images\image-20200417163000467.png)
  $$
  Loss=\sum_{i=1}^Nexp⁡(-y_i F_m (x_i ))=\sum_{i=1}^Nexp⁡(-y_i (F_{m-1} (x_i )+α_m G_m (x_i )))
  $$


  * 开始时，所有的样本权重相同，从第二轮开始，提高上一轮分错样本的权重，降低上一轮分对样本的权重，边界附近的样本点会得到更多的重视

  * 将各个弱分类器组成强分类器，分类误差小的弱分类器话语权大

* XGboost


  * 首先需要说一说GBDT，它是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。
  * 是一个树集成模型(加法模型)，将k个树的结果进行求和，得到最终的预测值
  * xgboost还支持线性分类器
  * 训练时，新的一轮加入一个新的函数，来最大化的降低目标函数

  ![](https://upload-images.jianshu.io/upload_images/5012681-7d96b3eaab4209af.png?imageMogr2/auto-orient/strip|imageView2/2/w/926/format/webp)

  * xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数，泰勒展开式对模型损失残差的近似

  * 缺失数据会被分到左子树和右子树分别计算损失，选择较优的那一个

* LightGBM

  * XGBoost使用基于预排序的决策树算法，每遍历一个特征就需要计算一次特征的增益，时间复杂度为O(data*feature)。
    而LightGBM使用基于直方图的决策树算法，直方图的优化算法只需要计算K次，时间复杂度为O(K*feature)

  * XGBoost使用按层生长(level-wise)的决策树生长策略，LightGBM则采用**带有深度限制的按叶子节点(leaf-wise)算法**。在分裂次数相同的情况下，leaf-wise可以降低更多的误差，得到更好的精度。leaf-wise的缺点在于会产生较深的决策树，产生过拟合。

  * min_data_in_leaf：这也是一个比较重要的参数，**调大它的值可以防止过拟合**，它的值通常设置的比较大。

  * 支持类别特征，不需要进行独热编码处理

  * 优化了特征并行和数据并行算法，除此之外还添加了投票并行方案

  * 采用基于梯度的单边采样来保持数据分布，减少模型因数据分布发生变化而造成的模型精度下降

  * 特征捆绑转化为图着色问题，减少特征数量

    

    

* GBDT

  * 梯度下降树，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树
  * 弱分类器一般选择CART TREE（分类回归树）（要求足够简单，并且是低方差和高偏差的）
  * 每轮迭代的时候，都去拟合损失函数在当前模型下的负梯度
  * CART TREE的生成过程实际上就是特征选择的过程，遍历每个特征，然后对每个特征遍历它所有可能的切分点，找到最优特征m的最优切分点j，最优即loss最小

  ![](https://images2017.cnblogs.com/blog/666027/201710/666027-20171031134114980-1949308807.png)

  * 如何用于分类

  

* Adaboost和GBDT的对比

  * boosting的策略不同树本质区别，Adaboost通过不断修改样本权重，不断加入弱分类器进行boosting，GBDT目标是不断减少残差，通过不断加入新的树来减少残差来建立新的模型

* XGBoost和GBDT的对比

  * GBDT的基分类器只支持CART树，而XGBoost支持线性分类器
  * GBDT在优化时只使用了一阶导数，而XGBoost对目标函数进行了二阶泰勒展开，（可以更为精准的逼近真实的损失函数）此外XGBoost支持自定义损失函数，只要损失函数二阶可导
  * xgboost在代价函数里加入了正则项，用于控制模型的复杂度，xgboost支持特征粒度上的并行

* 随机森林和GBDT的对比

  * 随机森林采用的bagging思想，而GBDT采用的boosting思想
  * 这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想
  * Bagging采用有放回的均匀取样，而Boosting根据错误率来取样
  * 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成
  * 组成随机森林的树可以并行生成；而GBDT只能是串行生成
  * 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来
  * 随机森林对异常值不敏感；GBDT对异常值非常敏感
  * 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成
  * 随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能

* xgb 和 lgb 的区别

  * xgb采用的是level-wise的分裂策略，对每一层所有节点做无差别分裂，而lgb采用了leaf-wise的策略，在当前所有叶子节点中选择分裂收益最大的节点进行分裂
  * xgboost使用 exact 算法，lightgbm使用了基于histogram的决策树算法
  * lgb在feature 和data 都做了并行

### 10. Language Model

* 定义：语言模型是对语句的概率分布的建模
* n-gram语言模型

![](https://www.zhihu.com/equation?tex=P%28w_1%2Cw_2%2C...%2Cw_n%29%3DP%28w_1%29P%28w_2%7Cw_1%29%5Ccdot%5Ccdot%5Ccdot+P%28w_n%7Cw_1%2C...%2Cw_%7Bn-1%7D%29)

* 马尔可夫假设，即当前词出现的概率只依赖于前n-1个词

![](https://www.zhihu.com/equation?tex=P%28w_i%7Cw_1%2Cw_2%2C...%2Cw_%7Bi-1%7D%29%3DP%28w_i%7Cw_%7Bi-n%2B1%7D%2C...%2Cw_%7Bi-1%7D%29)

* 第一代PTM通常采用浅层模型，尽管它们也可以捕捉单词的语义，但它们却不受上下文的限制，只是简单地学习共现词频，这样的方法明显无法理解更高层的文本概念，而第二代PTM专注于学习上下文的词嵌入，

* ELMO

  ![](https://img2018.cnblogs.com/blog/670089/201810/670089-20181021105842585-294009665.png)

  * 可得到上下文依赖的当前词表示

* GPT

  * 使用了transformer，在大数据集上使用语言模型作为附加任务效果更好，并且transformer比LSTM效果更好
  * 单向模型

* BERT

  ![](https://img2018.cnblogs.com/blog/670089/201810/670089-20181021105847524-1030716839.png)

  * MLM：随机挡上15%的词，进行预测
  * 预测下一个就句子
  * 双向特征的自编码预训练语言模型

* XLNET

  * BERT忽略了被mask位置之间的依赖关系，因此出现预训练和微调效果的差异
  * 自回归（单向），根据上文内容预测下一个可能跟随的单词根据上文内容预测下一个可能跟随的单词
    * 缺点是不能同时利用上文和下文的信息，ELMO的融合模式过于简单
    * 优点在于对于生成式的任务天然匹配
  * 通过对句子中的单词排列组合，把一部分Ti下文的单词排到Ti上文的位置
  * 对长文本类型的NLP应用占优势



### 11. 数据增强

* 加噪
  * 随机扔词，如随机在Embedding上加dropout
* Back Translation，反向翻译
* 同义词替换、插入、交换和删除（**同义词在NLP中通常具有比较相近的词向量，因此对于模型来说，并没有起到比较好的对数据增强的作用**）
* 句法、句子扩充、句子缩写
* 随机打乱文本词序

### 12. 排序

* ![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9Gdzk1WmpjU0tlNllyV3FKSkpZZlJUMnVFVHA2eWRoU2RjWGFkaWJCOW9SV0FnTDFXUWZaVThZcUhneDYzRnpHdXp3dTVVY0ZPb09NMldaTE91T0F0aWF3LzY0MA?x-oss-process=image/format,png)

* 冒泡排序

  * 重复走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来
  * 时间复杂度O(n^2)，稳定

  ~~~python
  def bubble_sort(nums):
      n = len(nums)
      for i in range(n-1):
          for j in range(n-i-1):
              if nums[j] > nums[j+1]:
                  nums[j],nums[j+1] = nums[j+1],nums[j]
      reutrn nums
  ~~~

  

* 快速排序

  * 从数列中挑出一个元素，称为“基准”,所有元素比基准小的摆在基准前面，比基准值大的摆在基准值后面，进行递归
  * 时间复杂度O(nlogn)，不稳定

  ~~~python
  ## 快速排序,平均时间复杂度为O(nlogn)，最坏时间复杂度为O(n^2),空间复杂度O(n)，不稳定
  
  def quick_sort(arr):
      if arr == []:
          return []
      temp = arr[0]
      left = quick_sort([i for i in arr[1:] if i <= temp])
      right = quick_sort([i for i in arr[1:] if i > temp])
      return left + [temp] + right
  ~~~

  

* 直接插入排序

  * 将数据插入到已经排好序的有序数据中
  * 时间复杂度为O(n^2)，稳定

* 希尔排序

  * 按下表的一定增量分组，对每组使用直接插入排序算法排序
  * 时间复杂度为O(n^1.3)，不稳定

* 直接选择排序

  * 在序列中找到最小元素，存放到排序序列的起始位置，然后再从剩余序列中寻找最大元素，放到排序序列的末尾

  ~~~python
  def selection_sort(arr):
       """选择排序"""
       # 第一层for表示循环选择的遍数
       for i in range(len(arr) - 1):
           # 将起始元素设为最小元素
           min_index = i
           # 第二层for表示最小元素和后面的元素逐个比较
           for j in range(i + 1, len(arr)):
               if arr[j] < arr[min_index]:
                   # 如果当前元素比最小元素小，则把当前元素角标记为最小元素角标
                   min_index = j
           # 查找一遍后将最小元素与起始元素互换
           arr[min_index], arr[i] = arr[i], arr[min_index]
       return arr
  ~~~

  

* 堆排序

  * 大根堆调整，建立大根堆，堆排序

  ~~~python
  def max_heapify(heap,heapSize,root):
      left = 2*root+1
      right = left + 1
      larger = root
      if left < heapSize and heap[larger] < heap[left]:
          larger = left
      if right < heapSize and heap[larger] < heap[right]:
          larger = right
      if larger != root:  # 如果做了堆调整则larger的值等于左节点或者右节点的值，这个时候做堆调整操作
          heap[larger],heap[root] = heap[root],heap[larger]
          # 递归的对子树做调整
          max_heapify(heap,heapSize,larger)
  
  
  def build_max_heap(heap): # 构造一个堆，将堆中所有数据重新排序
      heapSize = len(heap)
      for i in range((heapSize-2)//2,-1,-1): # 自底向上建堆
          max_heapify(heap,heapSize,i)
  
  def heap_sort(heap): # 将根节点取出与最后一位做对调，对前面len-1个节点继续进行堆调整过程。
      build_max_heap(heap)
      # 调整后列表的第一个元素就是这个列表中最大的元素，将其与最后一个元素交换，然后将剩余的列表再递归的调整为最大堆
      for i in range(len(heap)-1,-1,-1):
          heap[0],heap[i] = heap[i],heap[0]
          max_heapify(heap,i,0)
  ~~~

  

* 归并排序

  * 将n个元素平均划分为n/2个元素的子序列，递归解决两个规模为n/2的子问题，合并两个已排序的子序列

  ~~~python
  # 归并排序，平均时间复杂度O(nlogn)，最坏时间复杂度O(nlogn),空间复杂度O(n),稳定
  
  def merge(a, b):
      c = []
      h = j = 0
      while j<len(a) and h < len(b):
          if a[j] < b[h]:
              c.append(a[j])
              j += 1
          else:
              c.append(b[h])
              h += 1
      
      if j==len(a):
          for i in b[h:]:
              c.append(i)
      else:
          for i in a[j:]:
              c.append(i)
      return c
  
  def merge_sort(arr):
      if len(arr) <= 1:
          return arr
      
      mid = len(arr)//2
      left = merge_sort(arr[:mid])
      right = merge_sort(arr[mid:])
      return merge(left,right)
  ~~~

  ~~~python
  def merge(arr,start,mid,end):
      i = start
      j = mid
      while i<j and j<=end:
          if arr[i]>arr[j]:
              k = j
              key = arr[j]
              while k>i and arr[k-1]>key:
                  arr[k] = arr[k-1]
                  k -= 1
              arr[k] = key
              j+=1
          i += 1
          
  def merge_sort(arr,start,end):
      if start<end:
          mid = (start+end)//2
          merge_sort(arr,start,mid)
          merge_sort(arr,mid+1,end)
          merge(arr,start,mid+1,end)
          
  merge_sort(arr,0,len(arr)-1)
  ~~~

  

* 基数排序（非比较，可以突破基于比较排序的时间下限，以线性时间运行）

  * 按照个位数将数值分配至0-9编号的桶里，再按十位数分配到编号为0-9的桶里

* 计数排序（非比较）

* 基数排序（非比较）

* top k

~~~python
def selection_sort(arr):
     """选择排序"""
     # 第一层for表示循环选择的遍数
     for i in range(k):
         # 将起始元素设为最小元素
         max_index = i
         # 第二层for表示最小元素和后面的元素逐个比较
         for j in range(i + 1, len(arr)):
             if arr[j] > arr[max_index]:
                 # 如果当前元素比最小元素小，则把当前元素角标记为最小元素角标
                 max_index = j
         # 查找一遍后将最小元素与起始元素互换
         arr[max_index], arr[i] = arr[i], arr[max_index]
     return arr[:k]


# 前k个高频元素，前k大，小根堆
import heapq
class Solution:
    def topKFrequent(self, nums: List[int], k: int) -> List[int]:
        dic = {}

        for num in nums:
            dic[num] = dic.get(num,0)+1
        
        res = []

        for num in list(dic.keys())[:k]:
            res.append((dic[num],num))
        
        heapq.heapify(res)
        for num in list(dic.keys())[k:]:
            if dic[num] >res[0][0]:
                heapq.heappop(res)
                heapq.heappush(res,(dic[num],num))
        
        ans = [i[1] for i in res]
        return ans
    
import heapq
class Solution:
    def topKFrequent(self, nums, k: int):
        
        res = nums[:k]     
        heapq.heapify(res)
        for num in nums[k:]:
            if num > res[0]:
                heapq.heappop(res)
                heapq.heappush(res,num)        
        return res[0]
~~~

~~~python
# 前k小，大根堆，取反后用小根堆
import heapq
class Solution:
    def topKFrequent(self, nums, k: int):        
        res = [-i for i in nums[:k]]    
        heapq.heapify(res)
        for num in nums[k:]:
            if -num > res[0]:
                heapq.heappop(res)
                heapq.heappush(res,-num)
        
        res = [-i for i in res]
        return res
~~~

~~~python
def smallest_k(arr, l, r, k):
    if (k > 0 and k <= r - l + 1):
        index = partiton(arr, l, r)
        if (index - l == k - 1):
            return arr[index]
        elif (index - l > k - 1):
            return smallest_k(arr, l, index - 1, k)
        else:
            return smallest_k(arr, index + 1, r, k - 1 - index + l )


def partiton(arr, l, r):
    pivot = arr[r]
    i = l
    for j in range(l, r):
        if arr[j] < pivot:
            arr[i], arr[j] = arr[j], arr[i]
            i = i + 1
    arr[i], arr[r] = arr[r], arr[i]
    return i


array = [13,4,12,17,2,44,55,92,1,18,6]
print(smallest_k(array, 0, len(array) - 1, 8),array)
~~~

~~~python
# 堆排序

def findKthLargest(self,nums,k):
    
    def heapify(a,start,end):
        while True:
            max_pos = start #初始化最大值所在位置为目标所在
            if start*2+1 <= end and a[start] < a[start*2+1]:
                max_pos = start*2+1 # 如果左叶子节点存在，且大于目标值，则将最大值所在位置指向该节点所在位置
            if start*2+2 <= end and a[max_pos] < a[start*2+2]:
                max_pos = start*2+2 # 如果右叶子节点存在，且大于目标值，则将最大值所在位置指向该节点所在位置
            if max_pos == start:
                break # 如果目标即为最大，完成该节点堆化，跳出循环
            a[start],a[max_pos] = a[max_pos],a[start]
            start = max_pos
            
    for i in range(len(nums)//2-1,-1,-1):
        heapify(nums,i,len(nums)-1)
    i = len(nums)-1
    while i>len(nums)-1-k:
        nums[0],nums[i] = nums[i],nums[0]
        i -= 1
        heapify(nums,0,i)
    return nums[len(nums)-k]
~~~





### 13. 查找

* 顺序查找
  * 按照序列原有顺序对数组进行遍历比较查询
* 折半查找
  * 输入有序
  * 每次选取中间值，如果小于中间值，则再比较左边元素，否则比较右边元素
* 分块查找
  * 将n个数据分为m块，块内无序，按块有序
  * 选取各块中的最大关键字构成索引表，对索引表进行二分查找或顺序查找，以缺点待查记录在哪一块中，在已确定 的块中用顺序法进行查找



### 14. RNN，LSTM

* RNN
  * 优点：可以拟合序列数据
  * 在长文本的情况下，学习不到之前的信息
  * Teacher Forcing: 它每次不使用上一个state的输出作为下一个state的输入，而是直接使用训练数据的标准答案(ground truth)的对应上一项作为下一个state的输入

* LSTM
  * 通过遗忘门和输出门忘记部分信息来解决梯度消失问题，导数为累加形式，避免了梯度消失，通过设置阈值和使用正则化来解决梯度爆炸问题
  * 信息在过远的距离传播中损失很厉害
  * 无法很好地并行

* GRU
  * 参数少，收敛快，通常在数据集够大的情况下，选择LSTM效果应该会更好
  * 通常情况下两者效果相差不大，GRU训练更快

* 解决梯度消失的方法
  * 使用ReLU等激活函数
  * batchnrom，批规范化，每一层的输出规范为均值和方差一致的方法，消除了输入x带来的放大缩小的影响
  * 残差结构
  * LSTM

* CNN善于捕捉文本中关键的局部信息，而RNN则善于捕捉文本的上下文信息（语序信息），CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐

* seq2seq中的 Beam Search

  * greedy search，每次挑选最优结果

  * 参数，beam width，即每次挑选top B的结果

### 15. Transformer（attention）

* attention
* soft-attention：其选择的信息是所有输入信息在注意力 分布下的期望，hard-attention：取最高概率的输入信息
* 在计算 Attention 时主要分为三步，第一步是将 query 和每个 key 进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；然后第二步一般是使用一个 softmax 函数对这些权重进行归一化；最后将权重和相应的键值 value 进行加权求和得到最后的 Attention

![](https://img-blog.csdn.net/2018082211021619?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhaGFqaW5idQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

* self-attention

  ![](https://pic2.zhimg.com/80/v2-32eb6aa9e23b79784ed1ca22d3f9abf9_720w.jpg)

  ![](https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29+%3D+softmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V)

  * self-attention是attention的一种特殊形式,自注意力模型其实就是kqv是相等的,在序列内部做attention,一个句子的每个词都要对该句的所有词进行attention计算,目的是学习句子内部的依赖关系,捕获句子的内部结构

* 在encoder-decoder连接处,kv来自encoder,q来自decoder

* 假设Q和K都是独立的随机变量，满足均值为0，方差为1，则点乘后结果均值为0，方差为dk，也即方差会随维度dk的增大而增大，而大的方差导致极小的梯度，为了避免这种大方差带来的训练问题，论文中用内积**除以维度的开方**，使之变为均值为0，方差为1

* multihead就是有不同的QKV表示,最后将其结果结合起来

* add & norm:残差和归一化

* NLG(自然语言生成)

  * 基于模板,基于树

### 16. 调参

* 泛化误差,模型在测试集表现不好,过拟合和欠拟合都会让泛化误差高
* 激活函数的选择
  * 对于输出层,多分类任务选用softmax,二分类选择sigmoid,回归任务选择线性输出
  * softplus 是平滑版的Relu函数 f = log(1+e^x)
  * 对于中间隐层,优先选择relu(可以有效解决sigmoid和tanh出现的梯度弥散问题)
  * RNN要优先选用tanh激活函数
  * ![](https://bkimg.cdn.bcebos.com/formula/eea83acf5297707e5afae5dd910d1f49.svg)
  * ![](https://bkimg.cdn.bcebos.com/formula/ddafc46cc1b83d47066b5b3336a0bd74.svg)
* 学习率设定
  * 一般从0.1或0.01开始
  * 太大会导致训练不稳定,甚至出现Nan,太小会导致损失下降太慢
* 防止过拟合
  * L1,L2正则化,dropout,提前终止和数据集扩充
* 优化器选择
  * 如果数据稀疏,就用自适应方法,即Adagrad,Adadelta,RMSprop,Adam
  * SGD虽然能达到最小值,但是比其他算法用的时间更长,而且可能会被困在鞍点
* 残差块和BN层
  * 残差块可以让网络更深
  * BN层可以加速训练速度,有效防止梯度消失与爆炸,具有防止过拟合的效果
* 自动调参方法
  * Grid Search,网格搜索,在所有候选的参数选择中,通过循环遍历,尝试每一种可能
  * Random Search
  * Bayesian Optimization:贝叶斯优化,考虑到了不同参数对应的实验结果,迭代次数更少,速度更快
* 参数随机初始化与数据预处理
* 提升召回率
  * 增加数据量

### 17.单链表反转

* 就地逆序：定义前一节点,当前节点,后一节点三个指针
* 插入法逆序：从链表的第二个节点开始，把遍历到的节点插入到头节点的后面，直到遍历结束（不需要额外的变量）
* 递归：如果第一个节点后面的n-1个节点已经正确反转的话，只要处理第一第二个节点的指向关系就行了

~~~python
def reverse2(head):
        if head.next == None:  # 递归停止的基线条件
            return head
        new_head = reverse2(head.next)
        head.next.next = head	# 当前层函数的head节点的后续节点指向当前head节点
        head.next = None	# 当前head节点指向None
        return new_head
    
def reverse(head):
    if not head or head.next:
        return head
    pre,cur = None,head
    
    while cur:
        tmp = cur.next
        cur.next = pre
        pre,cur = cur,tmp
    return pre
        
~~~



### 18.判断两个数组是否存在相同的数字

* 遍历其中一个数组,在第二个数组使用二分查找
* 设置两个指针,顺序比较

### 19. Trie树(字典树)的建立和查找

* trie是一颗存储多个字符串的树,相邻节点的边代表一个字符,树的叶节点则代表完整的字符串

![](http://blog.chinaunix.net/attachment/201307/18/28977986_1374115884WXdK.jpg)

* 构建trie的基本算法为:逐一把每则单词的每个字母插入trie,如果存在就共享,不存在就构建对应的边和权重

### 20. 文件40亿+无重复数字，排序到新文件

* 使用位图bitmap,一个bit存储一个数字,那么40亿数据约为40亿bit约为500M内存
* 表示为32位的二进制,然后进行查找

### 21. python 可变和不可变类型、垃圾回收

* 指内存中的那块内容（value）是否可以被改变
* 可变:列表、字典，不需要再在其他地方申请内存
* 不可变：数字，字符串，元组，不可变类型在对对象本身操作的时候，必须在内存中申请新一块区域
* 垃圾回收机制：
  * 引用计数，若此对象无其他对象引用，则立马回收
  * 标记-清除，将所有容器对象放到双向链表中，循环遍历链表，引用计数回收

### 22. 机器学习算法（损失函数）

* EM算法（期望最大化算法）

  * 第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；
  * 第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

* 决策树（生成清晰的基于特征选择不同预测结果的树状结构）

  * 非参数学习算法，用于解决分类问题和回归问题

  * 每个内部节点表示一个属性上的判断，每个分支代表判断结果的输出，最后每个叶节点代表一种分类结果

  * 比较常用的决策树有ID3（用熵增原理来决定哪个做父节点，哪个节点需要分裂），C4.5（ID3分割太细可能过拟合，优化项要除以分割太细的代价，这个比值叫做信息增益率）和CART（分类回归树，选择GINI指数最小方案，总体内包含的类别越杂乱）

  * 何时停止分裂：在每个叶子节点上设置训练输入量的最小阈值，或者设置模型的最大深度

  * 信息增益

    * 信息增益

      ![](https://pic1.zhimg.com/80/v2-23a5d47dacea8d9581833b7fdd1896a6_720w.jpg)

      * 信息增益表示在条件A确定的情况下，信息的不确定性减少的程度，信息增益越大代表特征越好

    * 信息增益率

      * 信息增益率 = 信息增益/分裂信息

      ![](https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317133854323-1046650322.png)

      ![](https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317135209885-3830592.png)

    * 基尼指数

      * 基尼指数 = 样本被选中的概率 * 样本被分错的概率

      ![](https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317141739932-1276625834.png)

      ![](https://img-blog.csdn.net/20180322075101449?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L2UxNTI3Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

    * KL散度

      * 衡量两个概率分布的匹配程度，两个分布差异越大，KL散度越大

      ![](https://upload-images.jianshu.io/upload_images/1717593-3a0111e622aefc53.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/328/format/webp)
    
  * 优点

    * 易于理解和实现
    * 能同时处理数据型和常规型的属性
    * 对缺失值不敏感
    * 可以处理不相关的特征
    * 效率高，只需要一次构建，反复使用

  * 缺点

    * 容易过拟合，泛化性差
    * 忽略属性之间的相关性
    * 对新增加的样本，需要重新调整树结构

* 朴素贝叶斯分类器（比较容易解释，且不同维度之间的相关性小的模型，比如垃圾邮件过滤器）

  * **假设各个特征之间相互独立**

  ![](https://pic1.zhimg.com/80/v2-a2a73f43adcbb0bf4b9bae19b9495f81_720w.jpg)
  
  * 优点：所需估计的参数少，对缺失数据不敏感
  * 缺点：需要假设属性之间相互独立，需要知道先验概率
  
  ![](http://www.zhoulujun.cn/zhoulujun/uploadfile/images/2017/0913/20170913192938939781684.jpeg)


  * 先验概率p(类别)，p(特征)

  * 后验概率：事情已经发生，要求这件事情发生的原因是某个因素引起的可能性大小

* 最小二乘法

  * 线性回归

  ![](https://www.zhihu.com/equation?tex=%0A%5Cbegin%7Bcases%7D%0A%20%20%20%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20a%7D%5Cepsilon%3D2%5Csum%20(ax_i%2Bb-y_i)x_i%3D0%5C%5C%0A%20%20%20%20%5Cquad%5C%5C%0A%20%20%20%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20b%7D%5Cepsilon%3D2%5Csum%20(ax_i%2Bb-y_i)%3D0%0A%5Cend%7Bcases%7D%0A)

* 逻辑回归（LR）

  * 使用sigmoid函数作为判别函数

  ![](https://upload-images.jianshu.io/upload_images/4155986-f7814da0a2442bb3.png?imageMogr2/auto-orient/strip|imageView2/2/w/380/format/webp)

  * 使用交叉熵作为损失函数

  ![](https://img-blog.csdnimg.cn/20181110222213701.jpg)

  * 为什么用交叉熵做损失函数？ 通过极大似然估计推导得到的

  $$
  L(\theta)=\prod_{i=1}^{m}p(y|x;\theta) = \prod_{i=1}^{m}h_\theta(x_i)^{y_i}(1-h_\theta(x_i))^{1-y_i}
  $$

  

  * 缺点：容易欠拟合，分类精度不高；数据特征有缺失或者特征空间很大时表现 效果并不好
  * 用于分类场景， 尤其是因变量是二分类， 比如垃圾邮件判断（是/否垃圾邮件），是否患某种疾病（是/否）, 广告是否点击等场景

  * 第i个样本正确预测的概率

  ![](https://upload-images.jianshu.io/upload_images/4155986-c053e44c74a3e4f7.png?imageMogr2/auto-orient/strip|imageView2/2/w/826/format/webp)

  * 似然函数

  ![](https://upload-images.jianshu.io/upload_images/4155986-c6f3dcb85b8c89d8.png?imageMogr2/auto-orient/strip|imageView2/2/w/888/format/webp)

  ![](https://upload-images.jianshu.io/upload_images/4155986-f2324e145e45a0ab.png?imageMogr2/auto-orient/strip|imageView2/2/w/1024/format/webp)

  ![](https://upload-images.jianshu.io/upload_images/4155986-ca96b8cb282a4630.png?imageMogr2/auto-orient/strip|imageView2/2/w/1098/format/webp)

  ![](https://upload-images.jianshu.io/upload_images/4155986-4f3d52cd4064d4c8.png?imageMogr2/auto-orient/strip|imageView2/2/w/652/format/webp)

  

  

* SVM

  * 是一种二分类模型，是定义在特征空间上的间隔最大的线性分类器，基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面（任意点到平面的距离要超过γ）
  * 优点：

* LR和SVM

  * 都是分类算法

  * LR采用交叉熵损失（log损失），SVM使用合页损失

  * LR对异常值敏感，SVM对异常值不敏感

  * 训练集较小时，SVM比较适用，而LR需要较多的样本

  * LR尽量让所有点都远离超平面，而SVM只是让靠近中间分割线的那些点尽量远离

  * 在解决非线性问题时，SVM采用核函数的机制，而LR通常不采用核函数的方法（计算复杂度高）

    * 高斯核函数

      ![](https://img-blog.csdn.net/20140630143437953)

    * 多项式核

    ![](https://img-blog.csdn.net/20140707130435312)

    * 拉普拉斯核

    ![](https://img-blog.csdn.net/20140630143636250)

* KNN

  * 当预测一个新的值x的时候，根据它距离最近的K个点是什么类别来判断x属于哪个类别
  * KDTree

  

* 集成方法

  * bagging
  * boosting

* 聚类算法

  * 基于划分
  * 基于密度
  * 基于网格
  * 凝聚层次聚类
  * 谱聚类

* 主成分分析（PCA）

  * 数据降维，将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分
  * 选择最大差异性的主成分方向

* 奇异值分解（SVD）
  $$
  A = UΣV^T
  $$

  * U和V均为单位正交矩阵

* 独立成分分析（ICA）

  * X = As

* 潜在语义分析（LSA）


  * 构建了一个“单词-文档”矩阵，使用SVD的方法来寻找该矩阵的低阶近似

* 概率潜在语义分析（PLSA）


    * LSA得到的话题缺少可解释性
    * PLSA通过一个生成模型来为LSA赋予了概率意义上的解释
    * 每一个话题代表着一个不同单词上的概率分布，而每个文档又可以看成是话题上的概率分布，每篇文章都是由这样一个两层的概率分布生成的
    * 尽管PLSA模型在给定的文档上是一个生成模型，它却无法生成新的未知的文档，随着文档数量的增加，容易导致过拟合问题
    * 生成文档的过程
    
      * 按照概率p(d)选择一篇文档d
      * 根据选择的文档d，从主题分布概率中按照概率选择一个隐含的主题类别
      * 根据选择的主题，从词分布中按照概率选择一个词

* 潜在迪利克雷分配（LDA）


    * 每一篇文档都是有限个给定话题中的概率分布
    * 生成文档的过程
    
      * 按照先验概率p（d）选择一篇文档
      * 从Dirichlet分布α中取样生成文档d的主题分布![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_i)由超参数为α的Dirichlet分布生成
      * 从主题多项式分布中取样生成文档d第j个词的主题z
      * 从Dirichlet分布β中取样生成主题z对应的词语分布
      * 从词语的多项式分布中采样生成词语w

* 损失函数

  * 0-1损失（感知机）
  * MSE（均方误差，线性回归）

  ![](https://image.jiqizhixin.com/uploads/editor/041422c3-5343-4df6-9156-5a16d35b6155/1536733519963.png)

  * MAE(平均绝对值误差)，MBE（平均偏差误差）
  * hinge loss

  ![](https://image.jiqizhixin.com/uploads/editor/177ac671-9e93-43ba-9cd7-9b0748298882/1536733519318.png)

  * cross-entropy loss（逻辑回归）

![](https://img-blog.csdn.net/20160402172100739)

  * 多分类交叉熵

  ![](https://www.zhihu.com/equation?tex=J%3D+-+%5Csum%5Climits_%7Bi+%3D+1%7D%5EK+%7B%7By_i%7D%5Clog+%28%7Bp_i%7D%29%7D)

  * exponential loss（指数损失，boosting）

  ![](https://img-blog.csdn.net/20180821192935130?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Noa2F5Mzk5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 23. CRF

* 序列化标注算法
* 逐帧softmax将序列标注看成是n个k分类问题，后者将序列标注看成是1个k^n分类问题，假设分布是指数分布

![](https://image.jiqizhixin.com/uploads/editor/14997655-19f2-4a46-8960-acc3db91996a/1527042479792.png)

* 假设输出之间的关联仅仅发生在相邻的位置，并且关联是指数加性的，也就是说，我们只需要对每一个标签和每一个相邻标签对分别打分，然后将所有打分结果求和得到总分。
* 线性链CRF，假设函数g与x无关

![](https://image.jiqizhixin.com/uploads/editor/7447232f-0455-4599-9dc3-e0aa3691e1e5/1527042480159.png)



* HMM（生成式模型）

  * 马尔科夫链：状态序列的每个状态值仅取决于前面有限个状态
  * 描述由一个隐藏的马尔科夫链随机生成的不可观测的状态序列，再由各个状态生成一个观测而产生观测随机序列的过程，属于生成模型
  * HMM**三要素**：初始状态链Π、状态转移概率矩阵A和发射概率矩阵
  * 三个假设：
    * 当前观测值只由当前隐藏状态决定
    * 当前隐藏状态由前一个隐藏状态决定
    * 隐藏状态之间转移概率不随时间改变
  * P(x,y) = P(y)*P(x|y)  转移概率\*发射概率
  * Viterbi Algorithm, 复杂度O（L|S|^2）
  * HMM会给从未出现过的（x,y）一个较高的概率，因为HMM对转移概率和发射概率分别独立建模
  * 能够计算出给定一个词和它可能的词性的联合概率分布

  ![](http://images0.cnblogs.com/blog/133059/201507/161450321576527.png)

  * 
  * HMM假设了两类特征，一是当前词词性与上一个词词性的关系（转移概率），以及当前词语和当前词性的关系（发射概率），HMM的训练过程就是在训练集中学习这两个概率矩阵
  * HMM思路就是要在已知要标注句子s的情况下，去判断生成标注序列的概率

  ![](https://img3.sycdn.imooc.com/5ae2c4f200010c7104170056.jpg)

  

* CRF可以看成是序列化的logistic regression（判别式模型），它的目标函数不仅考虑输入状态特征函数，而且考虑标签转移特征函数

![](https://pic2.zhimg.com/80/1d3f9cefc0de33cfebe71bbc237ccc6b_720w.jpg)

* 特征函数
  * 句子s
  * 句子s中的第i个单词
  * 要评分的标注序列给第i个单词标注的词性
  * 要评分的标注序列给第i-1个单词标注的词性

![](https://img1.sycdn.imooc.com/5ae2c4f10001a38e04980068.jpg)

* 生成式和判别式
  * 判别式模型是直接对P（Y|X）建模，即在特征x出现的情况下标记y出现的概率，对所有样本只构建一个模型，确认整体判别边界，观测到输入什么特征，就预测最可能的label，常见的有感知机、k近邻法、决策树、线性回归模型、线性判别分析、SVM和神经网络等等；
  * 生成式模型是对P（X，Y）建模,即特征x和标记y共同出现的概率，需要对所有的label都进行建模，最终选择最优概率的label作为结果，需要枚举所有可能的观察序列，常见的有HMM，朴素贝叶斯模型、高斯混合模型和LDA
  * 判别式模型是根据一只羊的特征可以直接给出这只羊的概率（比如logistic regression，这概率大于0.5时则为正例，否则为反例），而生成式模型是要都试一试，最大的概率的那个就是最后结果
* HMM和CRF的区别
  * HMM是生成模型，CRF是判别模型
  * HMM是概率有向图，CRF是概率无向图
  * HMM求解过程可能是局部最优，CRF可以全局最优
  * CRF概率归一化比较合理，HMM则会导致label bias的问题
* 为什么不用RNN？
  * 单方向的RNN没有考虑整个序列，Viterbi则考虑了整个序列
  * CRF等viterbi 可以明确地考虑输出之间的依赖关系
  * cost和error不一定相关联
* Viterbe解码，最优路径搜索算法
  * 原复杂度为 D^N ，可降为 N\*D\*D
  * 通过已知的可以观察到的序列，和一些已知的状态转换之间的概率情况，通过综合状态之间的转移概率和前一个状态的情况计算出概率最大的状态转换路径，从而推断出隐含状态的序列的情况
  * 从开始状态后每走一步，就记录下到达该状态的所有路径的概率最大值，然后以此最大值为基准继续向后推进，显然，如果这个最大值都不能使该状态成为最大似然状态路径上的结点的话，那些小于它的概率值就更没有可能了



### 24. 从上到下打印二叉树

* 保存当前层还没有打印的节点和下一层的节点
* 之字形打印：打印某一层节点时，把下一层的子节点保存到相应的栈里，如果当前为奇数层，则先保存左子节点，再保存右子节点，如果当前层为偶数层，则先保存右子节点再保存左子节点

### 25. 模拟退火算法

* 模拟退火会以一定概率跳出局部最优解

![](https://upload-images.jianshu.io/upload_images/5501600-829414a6d94e3565.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

### 26. 贪婪算法

* 在面对问题求解时，总是选择做出在当前看起来是最好的选择，不从整体最优上加以考虑，它所做出的仅仅是在某种意义上的局部最优解

### 27. 深度优先和广度优先

### 28. 迭代器与生成器，装饰器

* 迭代器
  * 迭代器是一个可以记住遍历的位置的对象
  * 迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束，迭代器只能往前不会后退
  * 迭代器有两个基本方法，iter()和next()
* 生成器
  * 使用yield的函数称为生成器
  * 生成器是一个返回迭代器的函数，只能用于迭代操作
* python中，dict以键值对的形式存储，查询插入的速度块但内存占用大，list是列表，查询插入速度慢但内存占用小，dict的内部实现是哈希表，用哈希函数给字典的键分配
* 装饰器
  * 修改其他函数的功能的函数
  * 装饰器本质上是一个Python函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。
* property，装饰器负责把方法变成属性调用，getter，setter

### 29. pytorch和tensorflow的区别

* tensorflow的计算图是静态的，计算图与外界的通信都是通过tf.Senssionobject 和 tf.Placeholder 执行，它们是在运行时会被外部数据替换的张量
* pytorch的计算图是动态的，没有会话接口和换位符，图会随着执行过程而改变和执行节点
* **tf.keras.Sequential** 模型是层的简单堆叠，无法表示任意模型，通过对 **tf.keras.Model** 进行子类化并定义自己的前向传播来构建完全可自定义的模型

### 30. word2vec

* word2vec是 predictive 的模型，
* 损失函数

![](C:\Users\qxf\AppData\Roaming\Typora\typora-user-images\image-20200422204213496.png)

* Glove是 count-based 的模型，损失函数

![](https://img2018.cnblogs.com/blog/1187314/201909/1187314-20190909161301973-534597710.jpg)

### 31. 最小的k个数

~~~python
def fun(lists,k):
    n = len(lists)
    for i in range(k):
        min_index = i
        for j in range(i+1,n-1):
            if lists[j] < lists[min_index]:
                min_index = j
        lists[i], lists[min_index] = lists[min_index], lists[i]
    return lists[:k]
        
fun([10,9,2,8,3,5,7,6],2)
~~~

~~~python
# 前k小，大根堆，取反后用小根堆
import heapq
class Solution:
    def topKFrequent(self, nums, k: int):        
        res = [-i for i in nums[:k]]    
        heapq.heapify(res)
        for num in nums[k:]:
            if -num > res[0]:
                heapq.heappop(res)
                heapq.heappush(res,-num)
        
        res = [-i for i in res]
        return res
~~~

~~~python
# 前k个高频元素，前k大，小根堆
import heapq
class Solution:
    def topKFrequent(self, nums: List[int], k: int) -> List[int]:
        dic = {}

        for num in nums:
            dic[num] = dic.get(num,0)+1
        
        res = []

        for num in list(dic.keys())[:k]:
            res.append((dic[num],num))
        
        heapq.heapify(res)
        for num in list(dic.keys())[k:]:
            if dic[num] >res[0][0]:
                heapq.heappop(res)
                heapq.heappush(res,(dic[num],num))
        
        ans = [i[1] for i in res]
        return ans
    
import heapq

class Solution:
    def topKFrequent(self, nums, k: int):
        
        res = nums[:k]     
        heapq.heapify(res)
        for num in nums[k:]:
            if num > res[0]:
                heapq.heappop(res)
                heapq.heappush(res,num)        
        return res
~~~

~~~python
# k小
def partition(arr,l,r):
    tmp = arr[r]
    i = l
    for j in range(l,r):
        if arr[j] < tmp:
            arr[i],arr[j] = arr[j],arr[i]
            i += 1
    arr[i],arr[r] = arr[r],arr[i]
    return i


def smallest_k(arr,l,r,k):
    index = partition(arr,l,r)
    if index-l == k-1:
        return arr[:index+1]
    elif index-l > k-1:
        return smallest_k(arr,l,index-1,k)
    return smallest_k(arr,index,r,k-1-index+l)
~~~

~~~python
# k大
def topk(nums,k):
    def partition(left,right):
        tmp = left
        i,j = left+1,right
        while i<=j:
            if nums[i]<nums[tmp] and nums[j]>nums[tmp]: # 反号
                a[i],a[j] = a[j],a[i]
            if a[i]>=a[tmp]: # 反号
                i += 1
            if a[j] <= a[tmp]: # 反号
                j -= 1
            a[j],a[tmp] = a[tmp],a[j]
            return j
        
        left,right = 0,len(nums)-1
        while True:
            index = partition(left,right)
            if index = k-1:
                return a[index] # a[:index+1]
            elif index < k-1:
                left = index + 1
            else:
                right = index - 1
~~~



### 32. 剪绳子

~~~python
def fun(n):
    if n < 2:
        return 0
    if n == 2:
        return 1
    if n == 3:
        return 2
    l = [0,1,2,3]
    for i in range(4,n+1):
        m = 0
        for j in range(1,i//2+1):
            m = max(m,l[j]*l[i-j])
    	l.append(m)
    return l[n]
~~~

### 33. 评估指标

* AUC（Area Under Curve）

![](https://img-blog.csdn.net/20171129170159366)

![](https://img-blog.csdn.net/20171129170442743)

* Accuracy
* 混淆矩阵

|              | 预测为正样本 | 预测为负样本 |
| ------------ | ------------ | ------------ |
| 标签为正样本 | TP           | FN           |
| 标签为负样本 | FP           | TN           |

$$
P = \frac{TP}{TP+FP},R=\frac{TP}{TP+FN}
$$

$$
F_{\beta}=\frac{(1+\beta^2)×P×R}{\beta^2×P+R}
$$

* ROC（Receiver Operating Characteristic）（受试者工作特征曲线）

  * 曲线中每一个点对应一个threshold
  * 横坐标为FPR（假正例，负样本中被预测错的比例）![](https://img-blog.csdn.net/20180723093319871?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Nob2NvbGF0ZV9jaHVxaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
  * 纵坐标为TPR（真正例，正样本中被预测对的比例，即召回）![](https://img-blog.csdn.net/20180723093136952?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Nob2NvbGF0ZV9jaHVxaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
  * 第一个点(0,1)，即FPR=0, TPR=1，这意味着FN（False Negative）=0，并且FP（False Positive）=0。意味着这是一个完美的分类器，它将所有的样本都正确分类。
  * 第二个点(1,0)，即FPR=1，TPR=0，意味着这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。
  * 第三个点(0,0)，即FPR=TPR=0，即FP（False Positive）=TP（True Positive）=0，可以发现该分类器预测所有的样本都为负样本（Negative）。
  * 第四个点(1,1)，即FPR=TPR=1，分类器实际上预测所有的样本都为正样本。

* AUC

  * ROC曲线下的面积
  * AUC的物理意义正样本的预测结果大于负样本的预测结果的概率
  * AUC的计算方法同时考虑了学习器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器做出合理的评价。AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价学习器性能的一个原因（因此，在正负样本数量不均衡的时候，比如负样本的数量增加到原来的10倍，那TPR不受影响，FPR的各项也是成比例的增加，并不会有太大的变化）

* NMI
  $$
  MI = \sum{p(x,y)log\frac{p{(x,y)}}{p(x)p(y)}}
  $$

  $$
  H=-\sum{p(x)logp(x)}
  $$

  $$
  NMI = \frac{2MI}{H(x)+H(y)}
  $$

* RI
  $$
  RI = \frac{TP+TN}{TP+FP+TF+FN}
  $$
  P，R，F1

* MAP

  * AP的平均值
  
* MCC（Matthews correlation coefficient 马修斯相关系数）

![](https://img-blog.csdn.net/20181008200005768?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dpbm55Y2F0dHk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 34. KMP算法

* 

### 35. 降维

* PCA

  * 主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分
  * PCA的工作就是从原始空间中顺序地查找一组相互正交的坐标轴，第一个新的坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴选择的是与第1、2个轴正交的平面中方差最大的，可以得到n个这样的坐标轴，选前k个
  * 通过计算矩阵的协方差矩阵，然后得到协方差矩阵的特征值和特征向量，选择特征值最大的k个特征所对应的特征向量组成的矩阵

* SVD

  ![](https://img-blog.csdn.net/20180609144311809?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Byb2dyYW1fZGV2ZWxvcGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

  

* LDA（Linear Discrimination Analysis）

* AutoEncoder

  * 输入层的输入feature的数量（也就是神经元的数量）要等于输出层。同时要保证输入和输出相等

* t-SNE（t-distributed stochastic neighbor embedding）

  * 将欧几里得距离转换为条件概率来表达点与点之间的相似度
  * 将高维分布点的距离，用条件概率来表示相似性，同时低维分布的点也这样表示

### 36. 知识蒸馏

* 通过引入与教师网络相关的软目标作为total-loss的一部分，以诱导学生网络的训练，实现知识迁移

### 37. Linux

* Tmux：终端复用器，将会话与窗口解绑，与screen类似
* nvidia-smi：查看GPU的使用情况
* grep：查找文件里符合条件的字符串‘
* gpustat
* top：用于实时显示process
* 进程和线程
  * 进程就是一个程序在一个数据集上的动态执行过程，由程序，数据和进程控制块三个部分组成
  * 线程是一个基本的CPU执行单元，程序执行的最小单位，由线程ID、程序计数器、寄存器集合和堆栈共同组成
  * 协程是一种用户态的轻量级线程
  * 一个线程只能属于一个进程，而一个进程可以有多个线程，资源分配给进程，同一进程的所有线程共享该进程的所有资源，CPU分给线程，真正在CPU上运行的是线程
  * threading
  * 死锁：两个或两个以上的进程或线程在执行过程中，因争夺资源而造成的一种相互等待的现象
* 并发和并行的区别
  * 并发指的是多个程序可以同时运行的现象，是经过上下文快速切换，使得看上去多个进程同时都在运行的现象
  * 并行的"同时"是同一时刻可以多个进程在运行(处于running)
* 进程间通信：管道，消息队列，信号量，共享存储，Socket，Streams
  * 信号量是一个计数器，可以用来控制多个进程对共享资源的访问，它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源，因此，主要作为进程间以及同一进程不同线程之间的同步手段
* 中断：程序执行过程中，遇到急需处理的事件时，暂时中止CPU上现行程序的运行，转去执行相应的事件处理程序，待处理完成后再返回原程序被中断处或调度其他程序执行的过程
  * 处理器硬件故障中断
  * 程序性中断（非法指令等等）
  * 自愿性中断
  * I/O中断
  * 时钟中断
  * 外部信号中断
* TCP拥塞
  * 原因：资源（带宽、交换节点的缓存、处理机）的需求>可用资源
  * 方法：慢开始、拥塞控制、快重传、快恢复
* TCP和UDP的区别
  * 基于连接与无连接
  * 对系统资源的要求（TCP多，UDP少）
  * UDP程序结构较简单
  * 流模式与数据报模式
  * TCP保证数据正确性，UDP可能丢包，TCP保证数据顺序，UDP不保证
* TCP三次握手
  * 两次握手只能保证单向连接是畅通的，只有经过第三次握手，才能确保双向都可以接收到对方的发送的 数据。
  * 首先客户端向服务器端发送一段TCP报文
  * 服务器端接收到来自客户端的TCP报文之后，结束LISTEN阶段
  * 客户端接收到来自服务器端的确认收到数据的TCP报文之后，明确了从客户端到服务器的数据传输是正常的，结束SYN-SENT阶段。并返回最后一段TCP报文。
* 哈希表是由 数组+链表+红黑树
* 红黑树

### 38. 并查集

~~~python
class UQF:
    def __init__(self,n):
        self.p = [i for i in range(n+1)]
    def find(self,x):
        if self.p[x] != x:
            self.p[x]=self.find(self.p[x])
        return self.p[x]
    def union(self,i,j):
        ir = self.find(i)
        jr = self.find(j)
        self.p[ir] = jr

while True:
    try:
        N,M = map(int,input().split())
        ufs = UQF(M)
        for _ in range(M):
            i,j = map(int,input().split())
            ufs.union(i,j)
        rel = set(ufs.find(x) for x in range(1,N+1))
        print(len(rel)-1)
    except:
        break

~~~

### 39. 树的遍历

* 树的定义

~~~python
class TreeNode(object):
     def __init__(self, x):
         self.val = x
         self.left = None
         self.right = None
~~~

* 树的最大深度

~~~python
class Solution(object):
    def maxdepth(self, root):
        if root is None:
            return 0
        return max(self.maxdepth(root.left), self.maxdepth(root.right))+1
~~~

* 前序遍历

~~~python
class Solution(object):
    def preorder(self, root):
        if root is None:56
            return ''
        print root.val
        if root.lef:
            self.preorder(root.left)
        if root.right:
            self.preorder(root.right)
~~~

* 中序遍历

~~~python
class Solution(object):
    def midorder(self, root):
        if root is None:
            return ''
        if root.lef:
            self.midorder(root.left)
        print root.val
        if root.right:
            self.midorder(root.right)
~~~

* 后序遍历

~~~python
class Solution(object):
    def endorder(self, root):
        if root is None:
            return ''
        if root.lef:
            self.endorder(root.left)
        if root.right:
            self.endorder(root.right)
        print root.val
~~~

* 层次遍历

~~~python
class Solution(object):
　　def graorder(self, root):
　　　　if root is None:
　　　　　　return ''
　　　　queue = [root]
　　　　while queue:
　　　　　　res = []
　　　　　　for item in queue:
　　　　　　　　print item.val,
　　　　　　　　if item.left:
　　　　　　　　　　res.append(item.left)
　　　　　　　　if item.right:
　　　　　　　　　　res.apppend(item.right)
　　　　　　queue = res
~~~

* 矩阵的dfs

~~~python
class Solution:
    def dfs(self,grid,i,j):
        if cur_i<0 or cur_j<0 or cur_i==len(grid) or cur_j==len(grid[0]) or grid[cur_i][cur_j]!=1:
            return 0
        grig[cur_i][cur_j]=0
        ans = 1
        for di,dj in [[0,1],[0,-1],[1,0],[-1,0]]:
            next_i,next_j=cur_i+di,cur_j+dj
            ans += self.dfs(grid,next_i,next_j)
        return ans
    
##################################
def dfs(A,i,j):
    if 0<=i<N and 0<=j<M:
        a1,a2,a3,a4 = 1,1,1,1
        if i+1<N and A[i+1][j] > A[i][j]:
            a1 = 1 + dfs(A,i+1,j)
        if i-1>=0 and A[i-1][j] > A[i][j]:
            a2 = 1 + dfs(A,i-1,j)
        if j+1<M and A[i][j+1] > A[i][j]:
            a3 = 1 + dfs(A,i,j+1)
        if j-1>=0 and A[i][j-1] > A[i][j]:
            a4 = 1 + dfs(A,i,j-1)
        return max(a1,a2,a3,a4)
    return 0
ans = 0
for i in range(N):
    for j in range(M):
        ans = max(ans,dfs(A,i,j))
    
~~~

* 矩阵的bfs

~~~python
class Solution:
    def bfs(self,grid):
        ans = 0
        for i,l in enumerate(grid):
            for j,n in enumerate(l):
                cur = 0
                q = [[i,j]]
                while q:
                    [cur_i,cur_j] = q[0]
                    q.pop(0)
                    if cur_i<0 or cur_j<0 or cur_i==len(grid) or cur_j==len(grid[0]):
                        continue
                    cur+=1
                    grid[cur_i][cur_j]=0
                    for di,dj in [[0,1],[0,-1],[1,0],[-1,0]]:
                        next_i,next_j = cur_i+di,cur_j+dj
                        q.append([next_i,next_j])
                ans = max(ans,cur)
#######################
ans = 0
for i,l in enumerate(A):
    for j,n in enumerate(l):
        cur = 1
        q = [[i,j]]
        while q:
            tmp = []
            flag = 0
            for qq in q:
                [cur_x,cur_y] = qq
                if 0<= cur_x < N and 0 <= cur_y < M:
                    if cur_x+1<N and A[cur_x+1][cur_y] > A[cur_x][cur_y]:
                        tmp.append([cur_x+1,cur_y])
                        flag = 1
                    if cur_x-1>=0 and A[cur_x-1][cur_y] > A[cur_x][cur_y]:
                        tmp.append([cur_x-1,cur_y])
                        flag = 1
                    if cur_y+1<M and A[cur_x][cur_y+1] > A[cur_x][cur_y]:
                        tmp.append([cur_x,cur_y+1])
                        flag = 1
                    if cur_y-1>=0 and A[cur_x][cur_y-1] > A[cur_x][cur_y]:
                        tmp.append([cur_x,cur_y-1])
                        flag = 1
            if flag == 1:
                cur += 1
            q = tmp
        ans = max(ans,cur)
print(ans)
~~~



* 二叉排序树（二叉搜索树，二叉查找树）：空树或者左子树上所有节点的值均小于根节点，右子树所有节点 的值均大于根节点，**中序遍历是一个递增数列**
* 平衡树：1）它必须是二叉查找树；2）每个节点的左子树和右子树的高度差至多为1
* 红黑树：
* B树：是一种树状数据结构，能够用来存储排序后的数据
* B+树：是一种多路搜索树
* 字典树（Trie树）
* 满二叉树：除叶子节点外的所有结点均有两个子节点
* 完全二叉树：1~h-1层的结点数达到最大，第h层的结点连续集中在左边
* 最小生成树算法
  * **Kruskal**算法（加边法）：把图中所有边按代价从小到大排序，所选的边连接的两个顶点ui,viui,vi,应属于两颗不同的树，直到所有顶点都在一颗树内或者有n-1条边为止
  * **Prim**算法（加点法）：每次迭代选择代价最小的边对应的点，加入到最小生成树中。算法从某一个顶点s开始，逐渐长大覆盖整个连通网的所有顶点

### 40. 幂集

~~~python
class Solution:
    def subsets(self, nums: List[int]) -> List[List[int]]:
        result = []
        result.append([])
        for i in nums:
            l = len(result)
            j = 0
            while j < l:
                result.append(result[j]+[i])
                j += 1
        return result 
~~~

### 41. 数组中未出现的最小正整数

~~~python
def missNum(arr):
    if not arr:
        return
    left = 0
    right = len(arr)
    while left < right:
        if arr[left] == left + 1:
            left += 1
        elif arr[left] <= left or arr[left] > right or arr[arr[left]-1] == arr[left]:
            arr[left] = arr[right-1]
            right -= 1
        else:
            tmp = arr[left]
            arr[left] = arr[arr[left]-1]
            arr[tmp-1] = tmp
    return left + 1
~~~

### 42. 连续子数组的最大和

* 动态规划

~~~python
class Solution:
    def maxSubArray(self, nums: List[int]) -> int:
        dp = [0]*len(nums)
        dp[0] = nums[0]

        for i in range(1,len(nums)):
            dp[i] = max(dp[i-1]+nums[i],nums[i])
        return max(dp)
~~~

### 43. 特征选择

* 过滤法（Filter）：对特征进行评分，设定阈值，挑选特征，如方差，相关系数和互信息等
* 包装法（Wrapper）：根据目标函数（预测效果评分），每次选择若干特征
* 嵌入法（Embedded）：使用机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数大小选择特征

### 44. 大数定律

* 切比雪夫大数定律(随着样本容量n的增加，样本平均数将接近于总体平均，不需要独立同分布)![](https://bkimg.cdn.bcebos.com/formula/d4b4cfab41146dfd426bfb4c8323ef56.svg)
* 伯努利大数定律(当n足够大时，事件A出现的频率将几乎接近于其发生的概率，即频率的稳定性)![](https://bkimg.cdn.bcebos.com/formula/6db89b8b8514537d346bba03b2ddecfc.svg)
* 辛钦大数定律![](https://bkimg.cdn.bcebos.com/formula/f089582e30c7157969cc4bfa11d0d3ce.svg)

### 45. 100层楼和2个鸡蛋

* 如果有两个鸡蛋，则第一个鸡蛋用来缩小查找的范围，因而把100层分成若干段，先利用一个鸡蛋来确定临界层所在的段，再利用另一个鸡蛋确定临界层，关键点在于如何分段
* 所以只要解出x+（x-1）+（x-2）+...+1>=100就行了，x最小为14

### 46. LDA

* 目标是找到每一篇文档的主题分布和每个主题中词的分布，在LDA中，我们需要假定一个主题数目K，这样所有的分布都基于K个主题展开
* LDA假设文档主题的先验分布是Dirichlet分布

$$
\theta_d = Dirichlet(\vec \alpha)
$$

* LDA假设主题中词的分布是Dirichlet分布

$$
\beta_k= Dirichlet(\vec \eta)
$$

* Gibbs采样：如果我们通过采样得到了所有词的主题,那么通过统计所有词的主题计数，就可以得到各个主题的词分布。接着统计各个文档对应词的主题计数，就可以得到各个文档的主题分布
* Gibbs采样流程
  *  选择合适的主题数K, 选择合适的超参数向量α,η
  * 对应语料库中每一篇文档的每一个词，随机的赋予一个主题编号z
  * 重新扫描语料库，对于每一个词，利用Gibbs采样公式更新它的topic编号，并更新语料中该词的编号。
  * 重复第3步的基于坐标轴轮换的Gibbs采样，直到Gibbs采样收敛。
  * 统计语料库中的各个文档各个词的主题，得到文档主题分布θd，统计语料库中各个主题词的分布，得到LDA的主题与词的分布βk。
* 对于新文档预测
  * 对应当前文档的每一个词，随机的赋予一个主题编号zz
  * 重新扫描当前文档，对于每一个词，利用Gibbs采样公式更新它的topic编号。
  *  重复第2步的基于坐标轴轮换的Gibbs采样，直到Gibbs采样收敛。
  *  统计文档中各个词的主题，得到该文档主题分布

### 47. 球面的均匀采样

* u、v在[-1,1]均匀采样
* 计算r^2 = u^2+v^2，如果r^2 >=1 则重新采样，直到r^2<1
* x = 2\*u\*sqrt(1-r^2)，y = 2\*v\*sqrt(1-r^2)，z = 1-2*r^2
* 球体的均匀采样（正方体均匀采样，直到在球体内为止）（圆的均匀采样也是一样）
* 对于圆：r = a，theta = Pi\*b/(4*a)

### 48. 线性代数

* 线性方程组解的情况
  * 无解：系数bai行列式为0
  * 唯一解：线性方程组的矩阵的列是du满秩的，假设矩阵是m*n，它的zhi秩等于n*
  * 无穷多解：线性方程组的矩阵的列是不满秩的，假设矩阵是m*n，它的秩小于n
* 特征值对应的特征向量就是理想中想取得正确的坐标轴，而特征值就等于数据在旋转之后的坐标上对应维度上的方差
* 对于多元函数f(X)，我们可以通过其Hessian矩阵（Hessian矩阵是由多元函数的二阶导数组成的方阵）的正定性来判断。如果Hessian矩阵是半正定矩阵，则是f(X) **凸函数**
* 而且对于凸子集C中任意两个向量![img](https://bkimg.cdn.bcebos.com/formula/d496a84ab14d80ac2c0f177be9449faf.svg)、![img](https://bkimg.cdn.bcebos.com/formula/c82b9f1d2525cc5ba5fabe1537b240ba.svg)有![img](https://bkimg.cdn.bcebos.com/formula/79d720f917c190173fb4c223a275f3b7.svg)成立。

### 49. 概率

* 同时抛出两枚骰子,将所得的点数的乘积作为得分,求得分的数学期望
  * (1+2+3+...+6)^2 * (1/36) = 12.25

### 50. 样本不均衡

* 随机欠采样和随机过采样
* 数据合成
* focal loss（加权）

![](https://pic4.zhimg.com/80/v2-e0f4dbcb05407e75f492e36f25b03023_720w.png)



### 项目自述

项目主要是通过收集新闻和社交媒体数据这两大块的实时数据，对正在发生的舆情事情进行处理和分析

* 怎么获取实时的数据？
  * 分布式的爬虫进行爬取，两大模块，新闻和微博
  * 新闻通过新闻学院提供的相关的 新闻网站 进行定时爬取，这个部分数据量不多，去重后每天一万多条，全部处理
  * 微博爬虫则通过三个部分定义，第一部分为活跃用户，活跃用户定义为过去一周内有被爬虫爬到的用户，动态维持一个活跃用户库，第二部分为用户定义的关键词，从微博搜索获取，第三部分为微博自身热搜、热门话题和微博等
  * 将数据存储在ES数据库中，根据爬取时间对数据进行流式处理，对微博数据使用一些规则进行筛选，包括删除某些关键内容为空的数据，处理某些异常数据，选择较长文本的微博，阈值设为50，选择有一定热度的微博，即排除掉没有任何点赞评论转发的微博，我们认为这是噪音，设置关键词白名单和黑名单，白名单为提前定义的一些关键词和从最近24小时的新闻数据获取的微博数据，黑名单为提前定义的一些娱乐性、个人相关、抽奖和广告微博，选择发布时间在最近24小时的数据，通过这一层的筛选可以将每天的数据量控制在约10万
* 对新闻文本做了什么处理？
  * 新闻主要是进行了命名实体识别和关键词提取
  * 命名实体识别是通过传统的基于字的bi-LSTM+CRF模型，对数似然作为损失函数，模型在经典的人民日报语料上进行训练，得到约0.88的f1，得到了较好的结果
  * 进行关键词提取实际上是对命名实体识别的一个补充，因为基于字的命名实体识别的优点在于不用分词，可以很好地识别一些未登录词，缺点就在于最后识别出来的实体可能不完整的问题，对于一些人名等等可能会出现多一个字少一个字的情况，所以我们通过jieba分词后用tf-idf的方法进行关键词提取，tf-idf方法其实是很强的baseline，TopicModel抽取的关键词过于宽泛，LDA模型中，每篇文档是一些主题构成的概率分布，实际效果并不好，可以对命名实体进行一个补充，如果提取出来的关键词和命名实体在字符上很接近又不太一样，以关键词为准，作为一种纠错的方法。
* 对微博文本做了什么处理？
  * 三个部分，关键词提取、情感分析和文本分类
  * 对于关键词提取，我们注意到hashtag是很重要的信息，对于有hashtag的我们提取hashtag，对于没有hashtag的，我们使用tf-idf数据进行关键词提取
  * 对于情感分析，我们自己标注了2万条微博数据，平均字长度87，平均长度52，未登录词约占48%，把未登录词拆成字，微博数据的特点在于短和乱，不像新闻数据那样规则和整齐，我们对文本中的emoji表情进行了替换，使用text-CNN的方法进行分类，CNN相对于RNN的优点在于结构简单，计算量少，训练数据快，能够捕捉文中的关键信息，模型使用预训练好的微博词向量，通过卷积，池化和全连接层输出结果，（词向量300维，序列长度128，卷积核数目256，卷积核尺寸2，3，5，batch_size 64，epoch 10，交叉熵损失），macro-f1约为0.7
  * 对于文本分类，我们使用的是fasttext模型，fasttext是一种快速的文本分类算法，标注数据来自于知微事件库爬取的微博数据，约12万条，分为七个类别（社会、娱乐，财经、国际、科技、体育和政务），考虑了用户昵称，准确率达到约0.84（lr=0.25，epoch=25）
* 如何进行聚类？
  * 新闻和微博事件聚类分开
  * 对微博数据进行流式处理，传统的聚类方法，一来是需要对类别数进行指定，二来需要进行反复迭代，这对于流式的大量数据是不适合的，所以考虑single-pass流式数据增量聚类方法，所谓single-pass聚类方法即为对数据进行流式处理，每来一条数据对数据所属事件进行判断，如果事件库内不存在所属事件，则新建事件，不对事件数进行指定，也不再进行迭代。
  * 首先考虑使用无监督的方法进行聚类处理，主要是分为两个部分，即微博和事件表示，以及相似度计算，考虑过用tf-idf向量和w2v词向量均值来表示微博和事件，根据实际运行效果发现tf-idf表示方法远好于w2v，因为w2v方法引入太多噪音，而且w2v更多地是表示一种语义相似，对于事件相似判断更多时候是一种相关匹配，不能很好地对文本和事件进行表示，而tf-idf相似度本质上还是关键词字符上的相似度，对于微博这种短而乱的文本来说，虽然很难保证召回率（即同一个事件下的微博都被聚合在一起），因为对于一个事件来说有很多表示方法，但可以保证一定的准确率（即被聚合在一起的基本属于同一事件）。
  * 考虑到无监督的方法只能通过控制阈值来调整模型，不能达到一个很好的效果，所以考虑了使用有监督的方法来计算相似度，进而学习到一种相似度计算方法，所以进行了相关的调研
  * 其实之前有一个相关的研究叫做TDT（topic detection and tracking），是在新闻文本上进行话题的发现和追踪，基本上使用无监督的方法可以得到较好的结果，但是对于微博这类社交媒体的短文本来说，适用于传统新闻文本的方法其实并不适用于目前的情况。
  * TDT主要又分为两种，specified 和 unspecified，即指定事件和不指定事件，指定事件其实就是类似于一个分类任务，比如提前定义10个事件，然后判断每个文本属于哪个事件，这是一个很典型的分类任务，使用传统的一些分类模型其实可以得到不错的效果，但是对于现实情况来说，我们不可能提前知道会发生哪些事件，所以不能用直接的文本分类去考虑这个问题，即不会提前指定事件
  * 实际运行过程中我们会有一个活跃事件库，即最近发生的一些事件，然后对数据和事件库中的数据进行比较匹配，然后决定该数据的所属，所以我们把这个任务转化为一个类似于文本匹配的任务，每次数据和事件库中的数据进行匹配，我们可以通过一个有监督的方法去学习到一个文本和事件的匹配模式
  * 我们选择的是一个比较简单的匹配模型，即每条微博文本通过一个双向的LSTM和attention之后得到一个微博的表示，然后通过标注数据，选取另一条微博进行编码，然后两条微博的表示通过cosine相似度来判断是否属于同一事件，但在实际运行过程中，如果计算所有微博两两相似其实计算量会非常大，所以我们考虑计算微博和事件的相似度，通过一个类似于指数平滑的方法对事件进行表示，即每次根据当前结果来更新事件表示。
  * 我们还可以对事件表示进行相似度计算来获取相似事件
  * 关于数据标注实际上还没有一个比较合适的标注标准，因为事件是一个很主观的定义，不同人或者不同应用场景下的事件粒度可能会差别很大，所以我参考了一篇关于推特事件发现的文章的标注方法，即通过某一时期的新闻百科数据定义一些事件，然后对这段时间内的数据进行标注，去除事件无关的信息，获取了微博一周约3万条数据，标注了250个事件(武汉暴雨，小红书下架，保时捷女车主，乔碧罗被封杀，台风，军运会)，根据文本的发布时间，产生了匹配文本对，进行训练。
  * 最后在标注数据上效果优于无监督的方法，在实际运行过程中效果也较好
  * 我们还在推特数据上进行了尝试（莫言获诺贝尔奖，美国总统大选，hiphop award），效果也不错
* 如何计算热度
  * 不仅仅是考虑事件下的文本数量，而是综合考虑微博数，微博的点赞评论转发数，发布人的粉丝数，微博类别（不同类别给与不同的权重，如降低娱乐类的权重），微博发布时间，爬取时间（时间上的衰减），来自新闻的热门关键词，包含这些关键词的微博给予较高权重，对评论数较多的微博进行反复爬取更新状态
* 关于图数据库
  * 聚类结果使用图数据库进行存储，图数据库其实对于复杂关系的查询比较方便，比如查询某条微博所属事件的相似事件下某一条微博的关键词
  * 同样的，我们还可以通过微博事件关键词和新闻事件关键词对微博和新闻进行链接，这个链接主要就是通过字符上的相似度来计算的
* 总结
  * 整体上来说，整个流程包括了流式数据的获取、处理、聚类和分析，通过筛选等方法保证实时性，通过将聚类问题转化为有监督的匹配问题，提高事件聚类的灵活性和准确性，有利于从事件的角度进行舆情分析。



### 疫情期间网民情绪识别

* 10万条标注数据，90万条未标注数据，包括发布时间，发布人账号，微博内容，微博图片和视频等内容，macro-F1，

* 负例P：0.56，R:0.71，F1：0.63，中性P：0.88，R：0.79，f1：0.84，正例P：0.65，R：0.69，F1，0.66，平均f1，0.71
* CNN，0.69，RNN，0.68



### 百度实体链接

面向中文短文本的实体链指任务，输入短文本，实体id和实体名称，offset，f1，

BiLSTM+CRF训练得到的实体识别，f1约为0.75

然后用LSTM-DSSM做匹配

0.6，130名

