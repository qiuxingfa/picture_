# 论文结构

## 1 introduction

社交媒体文本事件聚类的难点：

1. 推特文本较为杂乱
2. 事件较难定义
  3. 无法提前对事件进行定义
  4. 事件是变化的

创新点：

1. 从匹配的思路去学习到一种文本和事件的编码方式，通过得到的编码使用single-pass增量聚类的方法进行事件聚类，尝试寻找一种文本和事件的表示方法以及一种文本和事件的通用匹配模式，不用提前对事件进行定义，可以同时实现新事件的发现和旧事件的更新
 	2. 构造合适的匹配数据集，使得模型在实际数据流聚类过程中表现更加鲁棒
 	3. 根据事件关键词的变化对事件的表示进行动态更新
 	4. 使用BERT的方法对文本和事件进行表示
 	5. 将事件表示进行保存，相对于Bert原文直接拼接输入网络进行计算，如输入100个文本需要和事件库100个事件进行匹配，则需要在Bert上做10000次inference，参考sbert，如果对文本和事件进行分别表示时，则只需要将文本和事件分别通过BERT进行inference，然后再使用矩阵乘法计算相似度，然后对事件表示进行更新，实际只需要进行200次bert的inference，计算量大大减少, 通过Bert进行inference的次数从n^2降到了2n
  6. 在实际的聚类过程中，通过设置事件的活跃时间，根据事件是否活跃来挑选候选事件，那么每次就只需要考虑在当前时间下的活跃事件 ,一方面减少计算量，另一方面也提高了实际聚类的准确率
  7. 在聚类过程中通过文本和事件直接进行匹配,而不是对文本和文本进行匹配,从而降低了计算量,从O(n^2)降到了O(nm), m<<n

## 2 Related Work

无监督方法Umass[2]，以tf-idf向量来表示文本，以文本表示的平均值来表示事件，使用cosine相似度来表示相似度，

* [Sayyadi et al.](Sayyadi H, Hurst M, Maykov A (2009) Event detection and tracking in
  social streams. In: Third International AAAI Conference on Weblogs and
  Social Media)  proposed to transfer a data stream into a KeyGraph, which represents the keywords by nodes and connects the nodes if corresponding keywords cooccurred in a document, so that the communities in the graph represent events occurred in the data stream
* The graph-based approaches were further improved to use series of graphs for event detection, considering the ineffciency in managing a huge amount of data using a single graph [[16]](Saeed Z, Ayaz Abbasi R, Razzak MI, Xu G (2019) Event detection in twitter stream using weighted dynamic heartbeat graph approach. IEEE Computational Intelligence Magazine 14(3):29{38).
* [McCreadie et al.](McCreadie R, Macdonald C, Ounis I, Osborne M, Petrovic S (2013) Scalable distributed event detection for twitter. In: 2013 IEEE international conference on big data, IEEE, pp 543{549)  and [Nur'Aini et al.](Nur'Aini K, Najahaty I, Hidayati L, Mur H, Nurrohmah S (2015) Combination of singular value decomposition and k-means clustering methods for topic detection on twitter. In: 2015 International Conference on Advanced
  Computer Science and Information Systems (ICACSIS), IEEE, pp 123{128) showed that K-means clustering can be successfully used for event detection.
* [Nguyen et al.](Nguyen S, Ngo B, Vo C, Cao T (2019) Hot topic detection on twitter data streams with incremental clustering using named entities and central centroids. In: 2019 IEEE-RIVF International Conference on Computing and Communication Technologies (RIVF), IEEE, pp 1{6[)  represented documents as term frequency-inverse document frequency (tf-idf) vectors during the clustering
* [Chen et al. ](Chen G, Kong Q, Mao W (2017) Online event detection and tracking in social media based on neural similarity metric learning. In: 2017 IEEE International Conference on Intelligence and Security Informatics (ISI), IEEE, pp 182{184[) suggested a deep neural network based approach for event detection while preserving the semantics.(NSMED)
* [Chen et al.](Chen, G., Kong, Q., Mao, W.: Online event detection and tracking in social media based on neural similarity metric learning. In: IEEE International Conference on Intelligence and Security Informatics, pp. 182–184 (2017))  proposed a clustering-based approach using a similarity metric and low dimensional representations of events which were learned from a neural network with an attention mechanism.
* LSH: [Petrovic, Osborne, and Lavrenko](Saša Petrovi´c, Miles Osborne, and Victor Lavrenko. Streaming first story detection with application to twitter. In Human language technologies: The 2010 annual conference of the north american chapter of the association for computational linguistics, pages 181–189. Association for Computational Linguistics, 2010.) introduce a document pivot approach
  where Locality Sensitive Hashing is used to cluster document
* single-pass（有伪代码）：A most prevailing method is the single-pass algorithm proposed by [Yang et al](Y. Yang, T. Pierce, and J. G. Carbonell. A study on retrospective and on-line event detection. 1998.), where new-in articles are merged into an event or become a new event comparing its similarity with existing events against a threshold.
* Specified events detection identifies events of particular types with prior information  [10](T SakakiM OkazakiY Matsuo,"Earthquake shakes Twitter users: Realtime
  event detection by social sensors," in Proceedings of the 19th
  International Conference on World Wide Web, ACM, 2010, pp. 851860.[), [11](A Schulz, B Schmidt, T Strufe, "Small-scale incident detection based on microposts," in Proceedings of the 26th ACM Conference on Hypertext & Social Media. ACM, 2015, pp. 3-12.). 
* [Liu et al.](Liu, J., Chen, Y., Liu, K., Zhao, J.: Event detection via gated multilingual attention mechanism. Statistics 1000, 1250 (2018)) had a similar idea that they exploited a recurrent neural network with a cross-lingual attention gate to identify events from multiple languages
* Topic Model: The underlying assumption of the Topic modeling based approaches is that some hidden topics always can be seen in the tweets that are processed. Tweets are demonstrated as a combination of topics, and each topic has a probability distribution in the terms that are included in those tweets. The most used probabilistic topic model has been Latent Dirichlet Allocation (LDA) [Blei et al.](Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent Dirichlet allocation. J. Mach. Learn.Res. 3, 993–1022 (2003)) in which the topic distribution has a Dirichlet prior.Extracting good topics from the limited context is a challenge that should be solved because of to the limit on the length of a tweet. In addition, the topic modeling based approaches usually consider too much computational price as an effective issue in a streaming setting, and are not that effective in managing the events that are described in parallel [Aiello et al.](Aiello, L.M., et al.: Sensing trending topics in Twitter. J. IEEE Trans. Multimed.15(6), 1268–1282 (2013)).
* single-pass：Because single-pass is simple，quick and lowcost， it is widely used in topic detection and similar work．



* baseline: [EDCoW(2012)](Weng, J., Lee, B.S.: Event detection in Twitter. In: Fifth International AAAI Conference on Weblogs and Social Media (2011)  [Twevent](Li, C., Sun, A., Datta, A.: Twevent: segment-based event detection from tweets. In: Proceedings of the 21st ACM International Conference on Information and Knowledge Management, pp. 155–164. ACM (2012)[) , [NEED(2015)](McMinn, A.J., Jose, J.M.: Real-time entity-based event detection for Twitter. In: Mothe, J., et al. (eds.) CLEF 2015. LNCS, vol. 9283, pp. 65–77. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-24027-5 6) , and [MABED(2014)](Guille, A., Favre, C.: Mention-anomaly-based event detection and tracking in Twitter. In: 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014), pp. 375–382. IEEE (2014)), [Universal Sentence Encoder (2018)](Cer, D., Yang, Y., Kong, S.-y., Hua, N., Limtiaco, N., John, R. S., Constant, N., Guajardo-Cespedes, M., Yuan, S., Tar, C., et al. (2018). Universal sentence encoder. arXiv preprint arXiv:1803.11175.)



## 3 Data

1. 根据论文[1]对事件的定义，An event is a significant thing that happens at some specific time and place.
2. 论文提供了包括2012-10-10至2012-11-07约120M的推特语料，包括美国总统选举，飓风Sandy等事件，其中有约150k推特语料包含关联性判断的标签，包含504个事件，由于推特的限制，作者只提供了推特id，所以需要对数据进行重新爬取，由于时间较久，其中有很多推特无法获取到，最终我们获取到了其中约70k的推特语料，包含所有504个事件，事件分布极其不均衡，为了保证模型能够作者学习到一个通用的事件发现方法，我们从504个事件中随机挑选254个事件作为训练集，剩余200个事件作为测试集，也就是说，训练集和测试集的事件并无交集.
3.  在所有504个事件中，有154个事件持续时间小于四小时（约占30.6%），56个事件持续时间在4-24小时（约占11.1%），293个事件持续时间在1-3天（58.1%），只有1个事件的持续时间超过3天（75.5h）（Hundreds of protesters in Libya storm the grounds of the country's parliament building in Bani Walid.）,所以我们将事件的活跃时间设为3天
4. 匹配数据的构造
   1. 由于我们是 文本-事件的匹配思路，所以在构造匹配数据时同时需要文本和对应事件的数据，其中事件表示为事件下所有文本根据tf-idf方法所提取出来的排名靠前的关键词，并且事件关键词会随着时间变化
   2. 以训练集为例，我们对训练集数据根据时间顺序遍历，并同时更新事件库，构造 当前文本和当前事件库某一事件的匹配数据，事件库是随着时间动态变化的
   3. 如果将所有可能的匹配数据都考虑进来的话会存在数据量过大和数据极不平衡的问题，因为事件不匹配的情况远远多于事件匹配的情况，一般来说，可以采用随机挑选负例的方法来使得数据平衡（许多匹配任务都是使用这种方法），这种情况下训练出来的模型实际效果并不好，因为随机挑选的负例与正例相差较远，对于模型并没有挑战性，使得模型的鲁棒性并不好，而实际情况中可能存在当前文本对于不同事件匹配程度接近的情况，模型在这种情况下表现较差，所以我们提出在构造匹配数据集时就根据文本和事件的tf-idf相似度来挑选具有挑战性的负例，使得模型能够在较为复杂的情况下获得较为精准的结果，在最终的匹配数据集中,在所有负例中,50%的为随机挑选,50%为tfidf相似度最接近的负例
5. 数据

|       | number of events | number of tweets | number of matching-data |
| ----- | ---------------- | ---------------- | ----------------------- |
| train | 254              | 36541            | 73081                   |
| test  | 200              | 33283            | 66540                   |





## 4 Model

文本表示和相似度计算

使用fine-tune过的bert模型进行文本和事件表示

以cosine计算相似度（cosine相似度对于不同特征的地位是相同的，）

聚类策略：





## 5 Experiment

分为两个部分

1. 匹配数据集的评估，二分类
2. 聚类评估，包括NMI，ARI和P，R，F1等评估指标(https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
   1. NMI(Normalized Mutual Information)

$$
MI(X,Y)=\sum_{i=1}^{|X|}\sum_{j=1}^{|Y|}P(i,j)log(\frac{P(i,j)}{P(i)P^{'}(j)})
$$

$$
P(i,j)=\frac{|X_i\cap Y_j|}{N}
$$

$$
P(i)=X_i/N
$$

$$
P^{'}(j) = Y_j/N
$$

$$
NMI(X,Y)=\frac{2MI(X,Y)}{H(X)+H(Y)}
$$

$$
H(X)=-\sum_{i=1}^{|X|}P(i)log(P(i));H(Y)=-\sum_{j=1}^{|Y|}P^{'}(j)log(P^{'}(j))
$$

  2. ARI,P,R,F1

     A true positive (TP) decision assigns two similar documents to the same cluster, a true negative (TN) decision assigns two dissimilar documents to different clusters. There are two types of errors we can commit. A (FP) decision assigns two dissimilar documents to the same cluster. A (FN) decision assigns two similar documents to different clusters
     $$
     RI=\frac{TP+TN}{TP+FP+FN+TN}
     $$

     $$
     ARI = \frac{RI-E[RI]}{max(RI)-E[RI]}
     $$

$$
ARI = \frac{2(TP*TN-FN*FP)}{(TP+FN)(FN+TN)+(TP+FP)(FP+TN)}\\
P=\frac{TP}{TP+FP},R=\frac{TP}{TP+FN},F1=\frac{2PR}{P+R}
$$



示例

 真实标签：1，2，2，3，3，3，4，4，4，4

 预测标签：a，a，a，b，b，b，c，c，c，d

|      |  1   |  2   |  3   |  4   |      |
| :--: | :--: | :--: | :--: | :--: | :--: |
|  a   |  1   |  2   |  0   |  0   |  3   |
|  b   |  0   |  0   |  3   |  0   |  3   |
|  c   |  0   |  0   |  0   |  3   |  3   |
|  d   |  0   |  0   |  0   |  1   |  1   |
|      |  1   |  2   |  3   |  4   |      |

$$
MI = 0.1*log(\frac{0.1}{0.1*0.3})+0.2*log(\frac{0.2}{0.2*0.3})+0.1*log(\frac{0.3}{0.3*0.3})\\+0.3*log(\frac{0.3}{0.3*0.4})+0.1*log(\frac{0.1}{0.1*0.4})=1.09
$$

$$
H(x) = -0.3*log0.3-0.3*log0.3-0.3*log0.3-0.1*log0.1=-1.3139
$$

$$
H(y) = -0.1*log0.1-0.2*log0.2-0.3*log0.3-0.4*log0.4=-1.2799
$$

$$
NMI = \frac{2*MI}{H(x)+H(y)} = 0.84
$$


$$
N = C_{10}^2=45
$$

$$
TP = C_2^2+C_3^2+C_3^2=7\\
FP = C_3^2+C_3^2+C_3^2-TP=2\\
FN = C_2^2+C_3^2+C_4^2-TP=3
$$

|                          | 预测在同一类别内的数据对 | 预测不在同一类别的数据对 |
| ------------------------ | ------------------------ | ------------------------ |
| 实际在同一类别的数据对   | 7                        | 3                        |
| 实际不在同一类别的数据对 | 2                        | 33                       |


$$
P = 0.78\\
R =0.7\\
F1 = 0.74\\
ARI = 0.67
$$




2. baseline

   Umass，使用tf-idf向量来表示文本，使用文本表示的均值来表示事件

   AvgW2V

   NSMED

3. 使用不同的特征提取器对文本和事件进行表示

   1. Bi-GRU
   2. Bi-LSTM
   4. GAT+avg
   5. GAT+attn



在匹配测试集的效果

|                  | acc        | P          | R          | F1         |
| ---------------- | ---------- | ---------- | ---------- | ---------- |
| Bi-GRU+attention | 0.8596     | 0.8221     | 0.9181     | 0.8674     |
| Bert             | **0.9094** | **0.8592** | **0.9795** | **0.9154** |
| SBert-avg        | 0.8634     | 0.8183     | 0.9342     | 0.8792     |
| SBert-CLS        | 0.8611     | 0.8122     | 0.9396     | 0.8713     |
| SBert+attention  | 0.8687     | 0.8222     | 0.9410     | 0.8776     |

在聚类测试集的效果

|                   | NMI        | ARI        | P          | R          | F1         |
| ----------------- | ---------- | ---------- | ---------- | ---------- | ---------- |
| tf-idf            | 0.8491     | 0.3083     | **0.9526** | 0.1881     | 0.3142     |
| avg-Glove         | 0.7291     | 0.5100     | 0.4078     | **0.7433** | 0.5226     |
| Bert-unsupervised | 0.7233     | 0.2734     | 0.5708     | 0.1880     | 0.2829     |
| Bert              | **0.9009** | 0.6712     | 0.8735     | 0.5539     | 0.6779     |
| SBert-avg         | 0.8535     | 0.6894     | 0.8207     | 0.6074     | 0.6963     |
| SBert-CLS         | 0.8623     | 0.6764     | 0.8313     | 0.5802     | 0.6834     |
| SBert+attention   | 0.8569     | **0.6904** | 0.8262     | 0.6031     | **0.6972** |



## 6 Conclusion 

## 7 参考文献

