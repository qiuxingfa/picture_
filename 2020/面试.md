## 一些概念

### 0. 目录

1. [大数据处理](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#1-大数据处理)
2. [大数据存储](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#2-大数据存储)
3. [情感分析](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#3-情感分析)
4. [文本分类](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#4-文本分类)
5. [相似度计算](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#5-相似度计算)
6. [命名实体识别](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#6-命名实体识别)
7. [关键词提取](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#7-关键词提取)
8. [图数据库](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#8-图数据库)
9. [聚类算法](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#9-聚类算法)
10. [数据流](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#10-数据流)
11. [热点事件](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#11-热点事件)
12. [排序](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#12-排序)
13. [匹配](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#13-匹配)
14. [对话系统](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#14-对话系统)
15. [推荐系统](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#15-推荐系统)
16. [图神经网络](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#16-图神经网络)
17. [知识图谱](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#17-知识图谱)
18. [机器学习和深度学习基本算法](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#18-机器学习和深度学习基本算法)
19. [BERT](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#19-bert)
20. [Transformer](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#20-transformer)
21. [LSTM，GRU，Attention](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#21-lstmgruattention)
22. [CRF](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#22-crf)
23. [mongodb Neo4j](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#23-mongodb-neo4j)
24. [pytorch](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#24-pytorch)
25. [排序](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#25-排序)
26. [SQL](https://github.com/qiuxingfa/picture_/blob/master/2020/面试.md#26-sql)

### 1. 大数据处理

* 通过http服务获取，get，根据爬虫时间对数据进行流式处理，使用一些规则进行筛选，通过pandas处理，每小时约8万数据
* 输入处理：
  * 通过drop_duplicate删除id重复的行
  * 删除某些关键内容为空的数据，如原文，发布时间和id
  * 替换点赞转发评论数的异常值，对空数据进行填充
  * 选择原文或转发原文长度大于50的微博
  * 选择评论+原始评论>5的微博，即关键微博
  * 并且对当前批数据进行排序（sort_values），优先处理评论数多的微博
  * 设立黑名单，排除娱乐性太高、个人相关、抽奖、广告微博
  * 选取发布时间在24小时之内的微博
* 文本处理：
  * 提取hashtag，去除标点符号，使用jieba分词，去停用词，词频统计（词云）
  * 提取关键词，没有hashtag的使用tfidf挑选关键词4个
  * 情感分析 CNN
  * 文本分类 Fasetext
  * 相似度计算

### 2. 大数据存储

* ES数据库存储（ElasticSearch）
  * 接近实时的搜索平台，是一种非关系型数据库，最小单位是文档本质是一个JSON格式的文件
  * 通过HTTP接口处理数据，GET、PUT、POST和DELETE
  * 索引为倒排索引，支持全文搜索，可以过滤匹配和排序，查询时效性高，扩容能力有限
  * 缺点：最明显的就是字段类型无法修改、写入性能较低和高硬件资源消耗，灵活性不高
* ,爬虫逻辑：
  * 关注用户池（动态更新）约二十多万活跃用户，定义最近一周有更新的用户为活跃用户，反之为不活跃，除去
  * 用户定义关键词，从微博搜索获取
  * 热门微博（热搜，热门话题，热门微博等）

### 3. 情感分析

* 使用约两万标注的微博数据，极性分为正中负三类，使用text-CNN，f1约为0.69

* 数据：

  * 平均长度87.2，平均词长度52.65，平均去停长度30，包含131109个词，其中OOV约64000个，约48.8%，大部分是用户名、英文数字以及分词错误等信息，拆开成字以后，剩余约6526个未登录词，大部分为生僻字

* 流程torchtext：输入batch=32，每个batch以最长文本长度进行padding，epoch=10

* 输入--> 三个卷积核 --> ReLU --> Maxpool --> concate --> fc --> dropout --> softmax

* 考虑多通道，即不同词向量集成（论文中一个静态一个非静态）

* maxpool（**最大问题**）是一种常见的降采样方法，常用来对特征进行降维，减少过拟合。但未考虑特征的位置信息，只取了最高的特征，忽视了其他特征，（解决思路k-max pooling，chunk-max pooling）

* 卷积核尺寸（**影响较大**）过滤器大小一般取3-5，对于较长句子则取大一些，卷积核数量也有较大影响，一般取100-600，

* 正则化的作用较小，相比而言dropout有更好的效果

* 数据多为非正式文本，所以比赛可能适合用ERNIE模型来微调

  ![](https://img2018.cnblogs.com/blog/1182656/201809/1182656-20180919171652802-136261435.png)

* 利用多个不同的size的kernel来提取句子中的关键信息，结构简单，计算量少，训练速度快

* 使用预训练的词向量，维度300，序列长度128，卷积核数目256，卷积核尺寸5，词汇表大小80000，学习率e-3，batch_size 64，epochs 10，损失函数 交叉熵

* 词语级，句子级，篇章级

* 基于语义的情感词典的方法，给不同情感词不同的权重，进行加权和表示情感

* fasttext，textCNN，LSTM，LSTM+attn，BERT

* SOTA

  * IMDB（acc: 95.4，[ULMFiT](https://arxiv.org/pdf/1801.06146.pdf)）

  * semEval（f1:0.685, [LSTM+CNNs](https://www.aclweb.org/anthology/S17-2094.pdf)）
  * Yelp (Error 37.95, [Char-level CNN]())

* 存在的困难，颜文字，表情包，讽刺句，比较句，情绪分类

### 4. 文本分类

* 从知微爬取的微博数据，约12万数据，分为七个类别（社会、娱乐、财经、国际、科技、体育、政务），考虑用户名称，使用fasttext方法（去停，lr=0.25，epoch=25），总的约0.84的准确率

* fasttext

  * 是一种快速的文本分类算法，使用n-gram去预测指定类别，层次霍夫曼树

  ![](https://image.jiqizhixin.com/uploads/editor/baa82a46-3b94-4cd6-8eb7-c1ff7a015caf/57.jpg)

* fasetext，textCNN，textRNN，HAN（textRNN+attn），textRCNN（textRNN+CNN），DPCNN，BERT，CNN善于捕捉文本中关键的局部信息，而RNN则善于捕捉文本的上下文信息（语序信息）

* HAN（2017）

  ![](http://www.52nlp.cn/wp-content/uploads/2018/10/5-768x740.jpg)

  * 保留了文章的结构信息，基于attention的结构有很强的解释性
  * 先得到句子的summary，再得到文章的summary
  * 固定长度进行切分，关联性较强的可能被拆解为两个句子

* DPCNN（2017）

  ![](http://www.52nlp.cn/wp-content/uploads/2018/10/9-1.jpg)

  * 使用两层的等长卷积来捕捉较长距离的依赖，
  * 残差网络，缓解梯度消失
  * 固定feature map的数量
  * 7个模块
  * max_pool_size = 3, max_pool_strides = 2

* textRCNN

  ![](https://img-blog.csdnimg.cn/20181125184659501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQyNDgxMjc=,size_16,color_FFFFFF,t_70)

  * 将卷积层换为双向RNN

* NB模型，随机森林（RF），SVM，KNN分类

### 5. 相似度计算

* tf-idf（term frequency–inverse document frequency）

  ![](https://img-blog.csdn.net/20180807190512798?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

  ![](https://img-blog.csdn.net/20180807191126207?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

  * 没有考虑位置信息，生僻词的idf会比较高，对于文档中出现次数较少的重要人名和地名信息提取效果不佳

* avg word vectors

  * 没有考虑单词顺序，丢掉了词与词之间相关意思

* WMD(Word Mover's Distance)

  * 计算每个单词的权重
  * 计算pair-wise的单词距离
  * 找到一个单词匹配方式，使得累加带权重求和距离最小，这个最小距离就是文本相似度

* 欧式距离

  ![](http://webdataanalysis.net/wp-content/uploads/2011/10/Euclidean-Distance.png)

* 曼哈顿距离

  * d(i,j)=|X1-X2|+|Y1-Y2|

* 杰卡德相似度

* 匹配

* 编辑距离

  * 插入，删除，替换
  * 无法获取语义信息
  * 空间复杂度高

* simHash+汉明距离

  * 将文档转换为64位的字节
  * 两个码不同数的个数

* 余弦相似度

### 6. 命名实体识别

* 只是对新闻文本进行了命名实体识别，作为一个特殊的序列标注问题，基于字的bi-LSTM+CRF,人民日报预料，F1:0.88

![](https://github.com/Determined22/zh-NER-TF/raw/master/pics/pic1.png)
$$
\begin{array}{l} LossFunction =  - \log \frac{{{e^{{S_{{\mathop{\rm Re}\nolimits} alPath}}}}}}{{{e^{{S_1}}} + {e^{{S_2}}} + ... + {e^{{S_N}}}}}\\ LossFunction =  - ({S_{{\mathop{\rm Re}\nolimits} alPath}} - \log ({e^{{S_1}}} + {e^{{S_2}}} + ... + {e^{{S_N}}}))\\ LossFunction =  - (\sum\nolimits_{i = 1}^N {{x_{i,{y_i}}} + \sum\nolimits_{i = 1}^{N - 1} {{t_{{y_i},{y_{i + 1}}}}} }  - \log ({e^{{S_1}}} + {e^{{S_2}}} + ... + {e^{{S_N}}})) \end{array}
$$
总分-真实路径得分

* Stanford NER, HanNLP，NLTK
* HMM
* 判别式模型CRF
  * 它的目标函数不仅考虑输入的状态特征函数，而且还包含了标签转移特征函数，使用Viterbi算法来得到最优标签序列
  * 优点在于 其为一个位置进行标注的过程可以利用丰富的内部及上下文特征信息
* 汉语的命名实体识别比英文要困难，分词等

### 7. 关键词提取

* tf-idf是很强的baseline

* 对于中文来说，分词和词性标注的性能对于关键词抽取的效果很重要

* Topic Model主要问题是抽取的关键词过于宽泛，不能较好反映文章主题,在LDA模型中，每篇文档是一些主题构成的概率分布

* TextRank实际效果并不比TF-IDF更有优势，效率较低，这是一种基于词图模型的关键词抽取算法

  ![](https://image.jiqizhixin.com/uploads/editor/bd7b71a7-9342-4c72-9b56-67a7b7622143/21.png)

* NLPIR

* 有监督序列标注问题

### 8. 图数据库

* 支持海量复杂数据关系运算的数据库
* 数据库文件分为四大类来存储：标签、节点、属性、关系
* 适合用访问量不大的2B金融数据分析，知识图谱等场景，不太适合动态数据
* neo4j单条插入每分钟千条左右，单条插入很慢，批量导入数据需要借助csv导入工具，如果数据量较小，只在单机上测试调研，推荐neo4j

### 9. 聚类算法

* 基于模型的方法
* 基于划分的 方法k-means
  * 第一阶段，确定每一个样本所属的聚类，在这个过程中，聚类中心保持不变（E步估计隐含类别y的期望值）
  * 第二阶段，确定聚类中心，在这个过程中，每一个样本所属的类别保持不变（M步调整参数使得在给定类别y的情况下，极大似然估计P(X,Y)能够到达极大值）
* 基于密度的DBSCAN，OPTICS
* 基于网格的方法
* 凝聚层次聚类（自上而下，自下而上）
* 谱聚类
* single-pass，传统的聚类算法需要获取全部数据后进行迭代计算，计算量大，而且有些还需提前指定类别数目，不适合实时的数据流，而single-pass聚类方法是一种简单高效的数据流聚类方法
* 实时数据流具有连续，近似无限，时变、有序及快速流动等特性
* 主要环节：相似性衡量，聚类算法
* 评估

  * NMI

### 10. 数据流

* 特点：海量的、时序的、快速变化的和潜在无限的和高维的
* STREAM：
  * 采用批处理的方式
  * 没有考虑数据流的演变，无法给出一个anytime的回应，无法给出不同时间粒度的聚类结果等等
* CluStream（基于层次的方法）：
  * 分为联机的微聚类和脱机的宏聚类
* HPStream
  * 引进了投影聚类技术来处理高维的数据流
  * 使用衰减簇结构来保存历史数据
* DenSream（基于密度的方法）
  * 首先试图将新的数据点合并到最近邻的p-micro-cluster，若失败，则合并到最近的o-micro-cluster，并判断该o-micro-cluster是否可以转换为p-micro-cluster，如果仍失败，则新建o-micro-cluster
* D-stream（基于网格的方法）

### 11. 热点事件

* 从事件聚类的结果考虑
* 不仅仅是考虑事件下的文本数，而是综合考虑微博数，微博的点赞评论和转发，发布人的粉丝数，微博类别（不同类别的微博给不同的权重），发布时间，爬取时间，热门关键词，分为两大部分，数据库部分和实时部分，实时部分随时间变化，且重要微博会进行监控，实时更新点赞评论转发数

### 12. 排序

* 根据热度排序，事件由24小时内的微博进行表示，也就是说事件的热度是变化的
* 时间衰减，和当前时间的距离

### 13. 匹配

* 相似度

  * 最长公共子序列
  * 编辑距离
  * 词向量基数余弦相似度（tf-idf或w2v）
  * 杰卡德相似度

* 定义

  * pointwise（一对一）（适合召回场景，交叉熵）
  * pairwise（一对二，两个之中有一个更匹配，hingeloss，适合排序的场景）
  * listwise（一对多，标签表示query与文本的匹配程度）

* 分类

  * PI（paraphrase identification）
  * STS（semantic text similarity）
  * SSEI（sentence semantic equivalent identification）
  * IR-QA
  * Ad-hoc retrieval，典型的相关性匹配

* 评价指标

  * NDCG(Normalized Discounted cumulative gain)归一化折损累计增益
    * DCG：每个累计增益除以一个折损值，让排名靠前的结果越能影响后面的结果，1/log2(i+1)
    * 归一化：除以理想情况下最大的DCG值
  * mean Average Precision
    * P：precision，预测正确的个数/测试总个数
    * AP：average precision，每一类别P值的平均值
    * MAP：mean average precision，对所有类别的AP取均值

* 方法

  * representation-based，基于Siamese 网络，提取文本整体语义再进行匹配，

    * 表征层使用MLP、CNN、RNN、Self-attention、Transformer encoder、BERT均可
    * 匹配层使用点积、余弦、高斯距离、MLP、相似度矩阵
    * DSSM（2013，MLP）（采用词袋模型，失去了语序和上下文信息），CNN-DSSM（CNN），LSTM-DSSM，
    * MV-LSTM（2015）

    ![](https://upload-images.jianshu.io/upload_images/14621057-e433a2bbf54789d9.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

    * ARC-I(CNN)

    ![](https://img-blog.csdn.net/20180908103338869?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21lbG9uMDAxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

    * 缺点是失去语义焦点、易语义飘逸，难以衡量词的上下文重要性

  * HAR模型

  ![](C:\Users\qxf\AppData\Roaming\Typora\typora-user-images\image-20200324144316654.png)

    * HiNT模型

    ![](C:\Users\qxf\AppData\Roaming\Typora\typora-user-images\image-20200324144558590.png)

    

  * interaction-based，捕捉直接的匹配信号，将词间的匹配信号作为灰度图，再进行后续的建模抽象

    * 交互层
    * 表征层，CNN，S-RNN均可
    * ARC-II,Match-SRNN，K-NRM，DRMM，DeepRank
    * MatchPyramid（2016，CNN）

    ![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy96SGJ6UVBLSUJQaE1pYUQxdUxpYnpYZTd5VGljclpTS0phQkRlbHJzVE5Zcnd5SGdjVmZiUXN6UDJUVXljOXVpYU9iUFk1emJkaWJmOWZxS1M0cWtlVm1jdWljQS82NDA?x-oss-process=image/format,png)

    * ARC-II

    ![](https://www.pianshen.com/images/187/eb9f1102693453ced04d9a486b73fe83.JPEG)

    * MIX(2018)

    ![](C:\Users\qxf\AppData\Roaming\Typora\typora-user-images\image-20200324144721435.png)

    * **多通道文本特征**和**多层Attention机制**
    * unigrams, bigrams 和 trigrams
    * 三种不同的attention，包括idf，词性和位置
    * global matching 和 local matching

    * DRMM

    ![](C:\Users\qxf\AppData\Roaming\Typora\typora-user-images\image-20200324144841884.png)

    

### 14. 对话系统

* 问答型
  * 单轮的对话
* 任务型
  * 比如订机票
  * 系统会根据当前状态（state）和相应的动作（action）来决定下一步的状态和反馈
* 闲聊型
  * 规则
  * 生成模型
  * 检索方法
* 评价指标
  * BLEU
  * ROUGE，ROUGE-L，是计算最长公共子序列的长度



### 15. 推荐系统

![](https://pic1.zhimg.com/80/v2-6ccabbfbe183b7d54e39b6b3c73e3b6c_720w.jpg)

* 基于内容的推荐

* 基于协同过滤的推荐

  * 首先找到与此用户有相似兴趣的其他用户，然后将他们感兴趣的内容推荐给此用户

  * 基于内存的协同过滤（Memory-based CF）

    * 启发式的方法来进行推荐，一是如何选择合适的相似性函数来更好的度量两个项目或用户的相似性是关键，二是如何进行推荐，最简单的方法是基于大多数的推荐策略
    * item-based:根据UI矩阵计算列相似度,选择特定项目最相似的k个项目构成推荐列表(图书、电子商务和电影，适用于物品数明显小于用户数的场景)
    * user-based:根据UI矩阵计算行相似度,选择特定用户最相似的k个用户,推荐高频项目(适用于用户较少的场景，时效性强，新闻推荐)

  * 基于模型的协同过滤（model-based CF）

    * 损失函数+正则想
    * 神经网络+层
    * 图模型+圈

  * 基于矩阵分解的推荐

    * Tranditional SVD(矩阵是稠密的,矩阵内的元素非空,用均值或其他统计学方法来填充矩阵)
    * FunkSVD
    * BiasSVD

    * SVD++
    * BiasSVDwithU
    * LR模型，无法组合特征，依赖于人工的特征组合，使得表达能力受限
    * FM，通过隐向量做内积来表示组合特征

    $$
    \hat y(\mathbf{x}) = w_0+ \sum_{i=1}^d w_i x_i + \sum_{i=1}^d \sum_{j=i+1}^d w_{ij} x_i x_j
    $$

    $$
    \hat y(\mathbf{x}) = w_0+ \sum_{i=1}^d w_i x_i + \sum_{i=1}^d \sum_{j=i+1}^d ( \mathbf{v}_i \cdot \mathbf{v}_j ) x_i x_j
    $$

    

    * FFM，增加Field的概念，相同性质的特征放在一个field

    $$
    y(\mathbf{x}) = w_0 + \sum_{i=1}^d w_i x_i + \sum_{i=1}^d \sum_{j=i+1}^d (w_{i, f_j} \cdot w_{j, f_i}) x_i x_j
    $$

    $$
    \mathcal{L} = \sum_{i=1}^N\log\left(1 + \exp(-y_i\phi({\bf w}, {\bf x_i}))\right) + \frac{\lambda}{2} |\!|{\bf w}|\!|^2
    $$

    

    * Wide&deep

      ![](https://pic2.zhimg.com/80/v2-784867e808f7c82d481d06070a7994a9_720w.jpg)

      * wide:广义线性部分![](https://www.zhihu.com/equation?tex=y%3D%5Cbm%7Bw%7D%5ET%5B%5Cbm%7Bx%7D%2C%5Cphi%28%5Cbm%7Bx%7D%29%5D%2Bb)
      * deep：前馈神经网络![](https://www.zhihu.com/equation?tex=%5Cbm%7Ba%7D%5E%7Bl%2B1%7D%3Df%28%5Cbm%7BW%7D%5El+%5Cbm%7Ba%7D%5El+%2B+%5Cbm%7Bb%7D%5El+%29)

    * DeepFM

      * 分为FM部分提取低阶组合特征和DNN部分提取高阶组合特征

      $$
      \hat{y}=sigmoid(y_{FM}+y_{DNN})
      $$

    * NMF

      ![](https://www.biaodianfu.com/wp-content/uploads/2019/08/nfm.png)

    * AMF(Attentional Factorization Machines)

      ![](https://www.biaodianfu.com/wp-content/uploads/2019/08/afm.png)

* 基于混合的推荐

* 评测指标

  * 评分预测任务

  ![](https://pic1.zhimg.com/80/v2-acb5e19ce4b89097b44a730238ead140_720w.jpg)

  ![](https://pic4.zhimg.com/80/v2-274ff0e3283908a46ce93a83c20d31e7_720w.jpg)

  * Top_N

    ![](https://pic1.zhimg.com/80/v2-1ea952a4b6849a7d70126791a5bdd378_720w.jpg)

    ![](https://pic1.zhimg.com/80/v2-827a855ce747927068c40853e01eaed8_720w.jpg)

* 冷启动问题

  * 提供非个性化的推荐，如热门排行榜
  * 利用用户的注册信息
  * 利用用户的社交网络账号
  * 引入专家知识
  * 用户/物品属性

### 16. 图神经网络

* 尝试用关键词图的方法来表示事件，

* 拉普拉斯矩阵 L = D - A（D是顶点的度矩阵(对角矩阵)，A是图的邻接矩阵）

* 拉普拉斯矩阵的特征向量是傅立叶变换的基

  ![](https://img-blog.csdnimg.cn/20190416102941542.png)

* 为什么要用拉普拉斯矩阵

  * 拉普拉斯矩阵是对称矩阵，可以进行特征分解（谱分解），这就和GCN的spectral domain对应上了
  * 拉普拉斯矩阵只在中心顶点和一阶相连的顶点上（1-hop neighbor）有非0元素，其余之处均为0
  * 通过拉普拉斯算子与拉普拉斯矩阵进行类比

* 这些权重在图中的节点间共享，该操作与卷积核滤波操作类似，节点的聚合表征不包含它自己的特征！该表征是相邻节点的特征聚合

* graph embedding

  * DeepWalk 类似 SkipGram，node2vec，
  * 节点之间的信息并没有共享
  * GNN的目标就是学习到一种包含节点邻居信息的隐藏向量h

* GCN

  * 多层 GCN 可能导致节点趋同化，没有区别性

* GAN（graph attention）

![](https://image.jiqizhixin.com/uploads/editor/5f4d9ab5-618e-46be-9c25-aaf55963bb65/640.jpeg)

* graph auto-encoder

* graph generative network

* DKN（2018 WWW 微软）

  ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/GNpj5fw72EoeC0bSYoe4blSaSswAjzFiaiaU3iaGtkA3S8KZs1OS6lKQVsgqClXiaAdLBpk39KiaNSEBksepzvKUbyQ/640?wx_fmt=png)

  * 知识抽取-->实体链接-->KCNN-->注意力

  

### 17. 知识图谱

* 用图的方式来表示知识，实体为节点，关系为边，将知识结构化

* 应用：搜索推荐，智能问答，数据分析与决策模型构建

* 推荐

  * **依次学习**（one-by-one learning）。首先使用知识图谱特征学习得到实体向量和关系向量，然后将这些低维向量引入推荐系统，学习得到用户向量和物品向量

    * 首先进行知识图谱特征提取：实体链接-->知识图谱构造-->知识图谱特征学习
    * 构造推荐模型：CNN将词向量、实体向量和实体上下文向量等融合，然后通过注意力机制，判断用户对当前新闻的兴趣

    * 优点是特征学习和推荐模块相互独立，但没办法做端到端的训练

  * **联合学习**（joint learning）。将知识图谱特征学习和推荐算法的目标函数结合，使用端到端（end-to-end）的方法进行联合学习

    * 将推荐算法和知识图谱特征学习的目标融合，在一个端到端的优化目标进行训练

  * **交替学习**（alternate learning）。将知识图谱特征学习和推荐算法视为两个分离但又相关的任务，使用多任务学习（multi-task learning）的框架进行交替学习

* 知识图谱的构建，

  * CN-DBpedia，复旦知识工厂，三千多万三元组，九百多万实体
  * 清华的xlore，三千多万三元组，一千多万实体，transE和transR做文本分类，

* 知识表示与建模

  * RDF（有向标记图），三元组
  * 使用连续向量的方式表示知识能发现隐形知识和潜在假设，对复杂知识结构支持不够，可解释性差

* 知识表示学习

  * 基于图的算法计算复杂度高，可扩展性差，计算效率低，数据稀疏
  * TransE：将relation看作是从实体head到tail的翻译，使得h+r=t

  ![](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D%5Csum_%7B%28h%2C+%5Cell%2C+t%29+%5Cin+S%7D%5Csum_%7B%5Cleft%28h%5E%7B%5Cprime%7D%2C+%5Cell%2C+t%5E%7B%5Cprime%7D%5Cright%29+%5Cin+S_%7B%28h%2C+%5Cell%2C+t%29%7D%7D%5Cleft%5B%5Cgamma%2Bd%28%5Cboldsymbol%7Bh%7D%2B%5Cell%2C+%5Cboldsymbol%7Bt%7D%29-d%5Cleft%28%5Cboldsymbol%7Bh%7D%5E%7B%5Cprime%7D%2B%5Cell%2C+%5Cboldsymbol%7Bt%7D%5E%7B%5Cprime%7D%5Cright%29%5Cright%5D_%7B%2B%7D)
  $$
  d(h+l,t)=||(h+r)-t||_2^2\approx0
  $$

  

  * TransH：将h和t投影到r所在的超平面上

  $$
  d(h+r,t)=||(h-w_r^Thw_r)+d_r-(t-w_r^Ttw_r)||_2^2\approx0
  $$

  

  * TransR：将h和t映射到关系所处的关系空间

  $$
  d(h,r,t)=||h_r+r-t_r||_2^2=||hM_r+r-tM_r||_2^2\approx0
  $$

  

  * TransD，在transR的基础上，将关系的映射矩阵简化位两个向量 的积

  $$
  M_{rh}=r_ph_p+I^{m*n}
  $$

  $$
  d(h,r,t)=||M_{rh}h+r-M_{rt}t||_2^2\approx0
  $$

  * TransA，自适应的度量函数，更换度量函数，区别对待向量表示中的各个维度，增加模型表示能力

  $$
  d(h+l,t) = (h+r-t)^T W_r(h+r-t)  = (L_r |h+r-t|)^TD_r(L_r|h+r-t|))
  $$

  

  * TransSparse，自适应稀疏转换矩阵

  $$
  \theta_r=1-(1-\theta_{min})N_r/N_{r^*}
  $$

  $$
  d(h,r,t)=||h_p+r-t_p||_2^2=||M_r(\theta_r)h+r-M_r(\theta_r)t||_2^2\approx0
  $$

  

  * TransG，高斯混合模型
  * ERNIE，随机mask一些token和实体对齐对，基于对齐的token预测实体，使用transE作为输入，分为T-Encoder和E-encoder

  

* 实体识别与链接

  * 实体识别主要难点在于表达不规律且缺乏训练语料

* 实体关系学习

  * 限定域关系抽取，开放域关系抽取

* 事件知识学习

  * 事件识别和抽取
  * 事件检测与追踪（TDT）（基于相似度聚类，基于统计概率聚类）
  * 事件具有复杂的内部结构，事件表达是灵活的，标注语料规模小，数据稀疏

* 知识存储与查询

* 知识推理

  * 基于符号的推理
  * 基于统计的推理

* 通用和领域知识图谱

* 语义集成

* 语义搜索

* 基于知识的问答

### 18. 机器学习和深度学习基本算法

* linear regression

  ![](https://img2018.cnblogs.com/blog/856725/201903/856725-20190303225317608-462127381.png)

  ![](https://img2018.cnblogs.com/blog/856725/201903/856725-20190303225403953-1601026207.png)

  

  * 均方误差

* logistic regression

  * 假设变量y服从伯努利分布

  ![](https://www.zhihu.com/equation?tex=P%28y%3D1%7Cx%3B%5Ctheta%29+%3Dg%28%5Ctheta%5ETx%29%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ETx%7D%7D)

  * 在逻辑递归中，决策边界有

  ![](https://www.zhihu.com/equation?tex=%5Ctheta%5ETx%3D0)

  * 损失函数由交叉熵定义

  ![](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+-%5Cfrac%7B+1+%7D%7B+m+%7D%5B%5Csum_%7B+i%3D1+%7D%5E%7B+m+%7D+%28%7By%5E%7B%28i%29%7D+%5Clog+h_%5Ctheta%28x%5E%7B%28i%29%7D%29+%2B+%281-y%5E%7B%28i%29%7D%29+%5Clog+%281-h_%5Ctheta%28x%5E%7B%28i%29%7D%29%7D%29%5D)

* SVM

  * 思路是基于训练集在样本空间中找到一个划分的超平面,使得任意点到平面的距离大于1

  ![](https://bkimg.cdn.bcebos.com/pic/1c950a7b02087bf443bcae84fcd3572c11dfcf0f)

  ![](https://bkimg.cdn.bcebos.com/pic/7aec54e736d12f2ec70d6dd941c2d5628535687b)

  * ![](https://bkimg.cdn.bcebos.com/pic/11385343fbf2b21119e97c23c48065380dd78e9c)
  * 软边界![](https://bkimg.cdn.bcebos.com/pic/8d5494eef01f3a2996bb2da39725bc315c607c68)
  * 通过拉格朗日对偶

  ![](https://img-blog.csdn.net/20170923173537336?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  ![](https://img-blog.csdn.net/20170923173733435?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  ![](https://img-blog.csdn.net/20170923174321119?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  * 使用SMO算法求解目标函数，采用合页损失

* 朴素贝叶斯

  ![](https://bkimg.cdn.bcebos.com/pic/ca1349540923dd546fb6c9f2df09b3de9d824866)

  * 属性之间是相对独立的

* ROC曲线

  * 横轴负正类率（预测为正的结果中负类占总负类的比例），纵轴真正类率

* GBDT

  * 决策树的加法模型
  * 就是所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的残差(这个残差就是预测值与真实值之间的误差)
  * 具有较好的解释性和鲁棒性

* XGboost

  * 将k个树的结果进行求和，作为最终预测值
  * 与GBDT的不同
    * 加入了正则项来控制模型的复杂度
    * 对代价函数进行了二阶泰勒展开
    * 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器

  

  

  



### 19. BERT

* ![](https://pic1.zhimg.com/80/v2-d942b566bde7c44704b7d03a1b596c0c_720w.jpg)

* 为什么只用了Transformer的encoder部分（双向的），GPT是单向decoder，因为BERT是一个预训练模型，只要学到其中语义关系即可，不需要去解码完成具体的任务

* token embedding+segment embedding+position embedding(transformer里用正弦波模拟信号周期性变化，而bert里时直接训练得到的)

* pre-train task

  * Masked LM（10%被替换，10%不替换，80%换成[MASK]）
  * Next Sentence Prediction

* Fine-tunning

  ![](https://pic2.zhimg.com/80/v2-b054e303cdafa0ce41ad761d5d0314e1_720w.jpg)

* ERNIE 1.0 改进了masking策略，基于phrase和entity进行mask

* ERNIE 2.0 连续学习，在一个模型中顺序训练多个不同任务

* bert-wwm：mask词，BERT-wwm-ext增加了训练数据集也增加了训练步数

* RoBERTa：动态mask，把预训练数据复制10份，使用不同的mask方法，每份数据都训练N/10个epoch，连续输入多个句子，而不是做句子对的匹配，使用了更大的batch size，更多的数据，更长时间的训练

* XLnet：其实在内部，就是通过Attention Mask，把其它没有被选到的单词Mask掉，以自回归语言模型的形式同时考虑上下文

* UniLM：使用三种特殊的Mask的预训练目标（unidirectional prediction，bidirectional prediction，seuqnece-to-sequence prediction），从而使得模型可以用于NLG

* ALBERT：

  * 对Embedding因式分解（原768）O(V\*H) -->O(V\*E+E\*H)
  * 跨层的参数共享（全连接层与attention层都进行参数共享）
  * 句间连贯
  * 移除dropout（模型并没有过拟合）

* 自回归语言模型：单向

* 自编码语言模型：mask



### 20. Transformer

![](https://img-blog.csdnimg.cn/20190407193306430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70)

![](https://img-blog.csdnimg.cn/2019040719332630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70)

* self-attention
  * 有三个新向量，分别为k、q、v，这三个向量是通过embedding与一个矩阵相乘得到的，维度为（64，512）
  * 计算某个词对当前位置的词的相关性的大小，使用q分别与每一个k相乘，除以8，并通过softmax计算每个词对当前位置的相关性大小，然后把这个值和v相乘并相加，得到当前节点的self-attention的值
* multi-head attention
  * 8个self-attention
  * 拼接乘以一个矩阵，得到最终矩阵
  * encoder-decoder连接处，k，q来自encoder，v来自decoder
* global attention 和 local attention





### 21. LSTM，GRU，Attention

* LSTM

  ![](https://upload-images.jianshu.io/upload_images/6983308-169c41fa64ff202f.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

  * 遗忘门（决定应该丢弃或保留哪些信息）
    $$
    f_t = \sigma(W_f·[h_{t-1},x_t]+b_f)
    $$

  * 输入门（更新细胞状态）

  $$
  i_t = \sigma(W_i·[h_{t-1},x_t]+b_i)
  $$

  $$
  C_t = tanh(Wc·[h_{t-1},x_t]+b_c)
  $$

  $$
  C_t = f_t*C_{t-1}+i_t*C_t
  $$

  * 输出门（确定下一个隐藏状态的值）

  $$
  o_t = \sigma(W_o[h_{t-1},x_t]+b_o)
  $$

  $$
  h_t = o_t*tanh(C_t)
  $$

* GRU（去掉了细胞状态，使用隐藏状态来进行信息的传递）

  * 重置门（reset gate）（类似于遗忘门和输入门，决定了要忘记哪些信息以及哪些新的信息需要被添加）

  $$
  r_t = \sigma(W_r[h_{t-1},x_t])
  $$

  

  * 更新门（update gate）（用于决定遗忘先前信息的程度）

  $$
  z_t = \sigma(W_z[h_{t-1},x_t])
  $$

  $$
  h_{t1}=tanh(W_h[r_t*h_{t-1,x_t}])
  $$

  $$
  h_t = z_t*h_{t1}+(1-z_t)*h_{t-1}
  $$

  $$
  y_t = \sigma(W_oh_t)
  $$

* GRU参数更少更容易收敛，**数据集较大**的情况下LSTM性能更好

* 结构上GRU只有两个门而LSTM有三个门，GRU直接将hidden state传给下一个单元，LSTM则用memory cell把hidden state包装起来

* LSTM计算Ct时不对上一时刻的信息做任何控制，而是用forget gate

### 22. CRF

* 构建输入序列到输出序列的条件概率模型



### 23. mongodb Nei4j

* mongodb最大的特点是表结构灵活可变，字段类型可以随时改变，缺点是对于多表查询和复杂事务等不友好
* redis是一个热门的key-value数据库，简单和高性能，redis会把所有数据加载到内存中，比常规的数据库读写能力要高，但查询能力一般

### 24. pytorch

* 预处理数据/词表-写好模型-定义损失和优化器-训练-测试
* SGD
  * loss可能会震荡
* Adam
  * 在稀疏数据场景下表现较好，学习率会单调递减至0，训练过程可能提前结束
* L1L2正则化
  * L1正则化具有稀疏性，使某些值为0

### 25. 排序

| 排序算法 | 平均时间复杂度 | 最坏时间复杂度 | 空间复杂度 | 是否稳定 |
| -------- | -------------- | -------------- | ---------- | -------- |
| 冒泡排序 | O(n^2)         | O(n^2)         | O(1)       | 是       |
| 选择排序 | O(n^2)         | O(n^2)         | O(1)       | 不是     |
| 插入排序 | O(n^2)         | O(n^2)         | O(1)       | 是       |
| 归并排序 | O(nlogn)       | O(nlogn)       | O(n)       | 是       |
| 快速排序 | O(nlogn)       | O(n^2)         | O(logn)    | 不是     |
|          |                |                |            |          |
|          |                |                |            |          |
|          |                |                |            |          |
|          |                |                |            |          |

* 冒泡排序：对列表进行遍历，依次比较两个相邻的元素，如果后一个元素比前一个小，则交换两元素的顺序
* 选择排序：一开始从0~n-1的区间上选择最小值，将其放在位置0上，然后再1~n-1的区间选择最小值放在位置1上，重复过程直到剩下最后一个元素
* 插入排序：将位置k上的数和之前的数依次进行对比，如果位置k上的数更大，将之前的数向后移位，最后将位置k上的数插入不满足条件点，反之不处理
* 归并排序：先将数组分成很多个小的数组，然后再将有序的子序列合并，采用递归的方法实现
* 快速排序：选择一个比较数，将子序列中比这个数小的放在前面，比这个数大的放在后面

### 26. SQL

* ```SQL
  SELECT LastName,FirstName FROM Persons
  
  SELECT DISTINCT Company FROM Orders 
  
  SELECT * FROM Persons WHERE City='Beijing'
  
  SELECT * FROM Persons WHERE FirstName='Thomas' AND LastName='Carter'
  
  SELECT Company, OrderNumber FROM Orders ORDER BY Company
  
  INSERT INTO Persons VALUES ('Gates', 'Bill', 'Xuanwumen 10', 'Beijing')
  
  UPDATE Person SET FirstName = 'Fred' WHERE LastName = 'Wilson' 
  
  DELETE FROM Person WHERE LastName = 'Wilson' 
  
  SELECT TOP 2 * FROM Persons
  
  SELECT * FROM Persons
  WHERE City LIKE 'Ne%'
  
  SELECT * FROM Persons
  WHERE LastName
  BETWEEN 'Adams' AND 'Carter'
  
  SELECT LastName AS Family, FirstName AS Name
  FROM Persons
  
  
  ```

### 自我介绍

你好，我是邱兴发，我现在是复旦大学一名硕士研究生，我的专业是微电子，我目前主要做自然语言处理相关的研究，具体研究方向是基于神经网络的社交媒体文本的事件发现方法，主要是突破传统的完全无监督聚类的方法的局限性，将流式数据的聚类问题转化为文本-事件的匹配问题，通过标注数据，使用神经网络的方法寻找一种事件发现的通用模式，主要参加了一个基于神经网络的舆情系统的项目，主要负责的是对约200万/天新闻和微博数据进行实时的筛选和处理，进行命名实体识别和情感分析，并进行事件聚类，及时从中发现一些热点事件，对事件进行热度排序和总结，以上就是我的自我介绍，谢谢







---

## 百度

* 内推码：va631t



---

## 一些平台

### 研究型

* 百度AIG
* 阿里达摩院NLP
* 腾讯AI lab
* 网易伏羲AI实验室（和游戏场景结合）
* 京东AI lab（电商场景）
* 滴滴AI lab（语言模型，检索，翻译）

### 业务型

* 百度
  * 度秘（C端对话）
  * 智能客服（B端对话对话）
  * 大搜索（IR，NLP和Rankking）
  * 凤潮（搜索广告）
* 阿里
  * 智能服务（对话问答）
  * 淘宝（搜索推荐）
  * 达摩院AIlabs（天猫精灵等）
* 腾讯
  * 微信事业群
  * AI技术平台
* 网易
  * 有道事业部（文本挖掘、知识图谱、机器翻译）

