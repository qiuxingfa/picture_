# 2018.11.28

---
* 看了一下tensorflow官网上一些简单的教程，跑了一下代码
* 看了一篇关于用语言模型进行迁移学习的论文（ULMFiT）
* IP为158的服务器cuda和cudnn都已装好，给实验室每位同学都开通了一个账号，但是权限没还没有设置好
* 写了一个简单的爬虫脚本，能实现输入一个标签，爬取相应标签页面的微博信息并保存<br>

![](https://github.com/qiuxingfa/picture_/blob/master/2018.11/c0c2ec5f7a7b93d6abdc9b17f3fb845.png)<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

----
## [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)

* 受迁移学习在CV应用的启发，提出Universal Language Model Fine-tuning (`ULMFiT`)，在6项text classification task上达到state-of-the-art（2018年1月），[**open source**](http://nlp.fast.ai/ulmfit)
* fine-tuning pretrained word embeddings 也是一种简单的迁移学习，但很多任务只是把预训练好的embedding（词或者句子的向量，由不同层参数连接起来）当作固定的参数，限制了它们的作用，在CV领域，hypercolumns已经几乎被end-to-end fine-tuning所取代，与CV不同的是，NLP model通常都比较浅，所以需要不同的fine-tunning技巧：
    * 对不同任务有区别的 fine-tuning（$η^{l-1}$=$η^l/2.6$)
    * 斜三角式的learning rates(`STLR`)，
    * **Concat pooling**:$h_c=[h_T,maxpool(H),meanpool(H)]$(`保留原始信息`）,
    * 渐变式的unfreezing，避免在fine-tuning的过程中把之前所学到的知识全部消除<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.11/83fba435ebbd54c11d9f3c4c8330493.png)<br>
* ULMFiT选取了通用领域的语料（Wikitext-103，28595篇文章，103M单词）进行预训练（双向），然后在目标任务进行整个LM的微调，最后在目标分类任务进行逐步微调，新的fine-tuning方法进行训练，防止过拟合，模型即使在小的数据集上也能达到很好的效果<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.11/92b80438d73f876d43a840221f1d234.png)<br>
* 结果<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.11/532b8ed05d95957663c1d116dd77444.png)<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.11/ba45bd005dc6052335050aad4fa23ee.png)<br>


