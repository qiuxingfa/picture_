# 2018.11.21

[NLP-datasets](https://github.com/niderhoff/nlp-datasets/blob/master/README.md)

* 试了一下fasttext在IMDb数据集（50000条数据，正负两种感情）和amazon数据集（568454条数据，分五类）上的效果，IMDb准确率最好为89%，Amazon准确率最好为82%，目前其他模型在这两个数据集最好的效果大约都是95%，差距明显<br>

* 目前在IMDb数据集上表现最好的是今年1月的论文 [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf) 所提出的Language Model Fine-tuning(`ULMFiT`)的方法<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.11/92b80438d73f876d43a840221f1d234.png)<br>

* 实际上这种方法就是先用大量的语料无监督地训练一个语言模型，然后对有监督的任务进行微调的迁移学习方法，和谷歌的BERT的思想是一致的
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>


---

## [Probabilistic FastText for Multi-Sense Word Embeddings](http://cn.arxiv.org/pdf/1806.02901v1)

* 一词多义，字特征，n-gram一定程度上解决未登录词的问题，高斯混合密度，同时有`多义表达`以及`带丰富语义的稀有词`
* 这个模型在SCWS（用于区分不同词义）上取得了state-of-the-art,比W2GM大约有一个百分点的提升


### Probabilistic FastText
* w由（词典级的向量+n-gram向量）取平均值得到
* n-gram举例：
    * beautiful：<be, bea, eau, aut, uti, tif, ful, ul>

### Qualitative Evaluation 
* Nearest neighbors
* Subword Decomposition

-----
## [Multimodal Word Distributions](https://arxiv.org/pdf/1704.08424.pdf)

* 多模式表示（为解决一词多义问题）<br>
* ![](https://github.com/qiuxingfa/picture_/blob/master/2018.11/c403755aa57370bc261b1816583ec53.png)<br>
* 每个椭圆代表一个高斯组件，椭圆的中心由mean_vector决定，而曲面由协方差矩阵决定，左边是随机初始化的高斯混合表示，右边是训练后的高斯混合
* L(w; c; c') = max(0;m-􀀀 logE(w; c) + logE(w;c'))
(**max-margin**)
* one negative context word for each positive context word
* 相似度计算
    * Maximum Cosine Similarity
    * Minimum Euclidean Distance
* [Embedding Visualization](https://github.com/benathi/word2gm#visualization)<br>
    
    

