# 2018.9.15
---
## Neural Architectures for Named Entity Recognition

1. Unfortunately, languagespecific
resources and features are `costly` to develop
in new languages and new domains, making
NER a challenge to adapt<br>
2. LSTM-CRF Model
3. Transition-Based Chunking Model
* Chunking Algorithm
* Representing Labeled Chunks
4. InputWord Embeddings
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/611f748930c133d17e3e163df6e35f4.png)<br>
* Character-based models of words
* Pretrained embeddings
* Dropout training
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/fed10c6385229327740057af9376b41.png)<br>
5. Experiments
* Training
* Data Sets
* Results
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/684a6f6518396f998f9818da74144aa.png)<br>
6. code
[The code of the LSTM-CRF](https://github.com/glample/tagger)<br>
[The code of Stack-LSTM](https://github.com/clab/stack-lstm-ner)

## word2vec
来产生词向量的相关模型
[Word2Vec Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
[Word2Vec Resources](http://mccormickml.com/2016/04/27/word2vec-resources/)

1. One-hot Representation<br>
* “可爱”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...] 
* “面包”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]
* 向量的维度会随着句子的词的数量类型增大而增大
* 任意两个词之间都是孤立的，无法表示语义层面上词汇之间的相关信息，而这一点是致命的。
2. 词的分布式表示
* `Harris` 在1954年提出的“分布假说”为这一设想提供了理论基础：上下文相似的词，其语义也相似。`Firth` 在1957年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定。
![](http://attachbak.dataguru.cn/attachments/portal/201805/03/131555uwstwas4a6tkwzl8.jpg)
3. 词嵌入
* 基于神经网络的分布表示一般称为`词向量`、`词嵌入`（ word embedding）或`分布式表示`（distributed representation）。核心依然是上下文的表示以及上下文与目标词之间的关系的建模。
* 将vector每一个元素由整形改为浮点型，变为整个实数范围的表示；将原来稀疏的巨大维度压缩嵌入到一个更小维度的空间
* 词向量是训练神经网络时候的隐藏层参数或者说矩阵。
![](http://attachbak.dataguru.cn/attachments/portal/201805/03/131556ohn99d9onh2np93k.jpg)<br>

        ![](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)
4. 两种训练模式
* CBOW (Continuous Bag-of-Words Model)<br>
上下文来预测当前词
![](http://attachbak.dataguru.cn/attachments/portal/201805/03/131556wjk9opzrktwpdttj.jpg)

* Skip-gram (Continuous Skip-gram Model)<br>
当前词预测上下文
![](http://attachbak.dataguru.cn/attachments/portal/201805/03/131556rqqoke9o4ujzufjq.jpg)
5. 加速方法
[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)<br>
* Treating `common word pairs` or `phrases` as single “words” in their model.
* `Subsampling frequent words` to decrease the number of training examples.<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/bfde864b66e81146ebc712b0a5e573a.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/12a6ddabfab42c4ed8fb2b23482fe6b.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/48fa21c69323c6dde966ce5e4c8bf19.png)<br>
* Modifying the optimization objective with a technique they called “`Negative Sampling`”, which causes each training sample to update only a small percentage of the model’s weights<br>
        Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!<br>
* Negative Sample
>把语料中的一个词串的中心词替换为别的词，构造语料 D 中不存在的词串作为`负样本`。在这种策略下，优化目标变为了：较大化正样本的概率，同时最小化负样本的概率。<br>

    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/1f417de699cc55846cf6cf80cf78660.png)

* Hierarchical Softmax
>Hierarchical Softmax是一种对输出层进行优化的策略，输出层从原始模型的利用softmax计算概率值改为了利用Huffman树计算概率值。一开始我们可以用以词表中的全部词作为叶子节点，词频作为节点的权，构建Huffman树，作为输出。从根节点出发，到达指叶子节点的路径是的。Hierarchical Softmax正是利用这条路径来计算指定词的概率，而非用softmax来计算。即Hierarchical Softmax：把 N 分类问题变成 log(N)次二分类

6. word2vec和word embedding的区别
* word2vec是谷歌提出的一种word embedding的具体手段，采用了两种模型(CBOW与skip-gram模型)与两种方法(负采样与层次softmax方法)的组合，比较常见的组合为skip-gram+负采样方法
* Word embedding的训练方法大致可以分为两类<br>
        * 无监督或弱监督的预训练 
        * 端对端（end to end）的有监督训练

7. Learning Phrases
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/75e019f1b267bb4472b14646f2be512.png)

        
        




