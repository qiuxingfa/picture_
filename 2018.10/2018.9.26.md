# 2018.9.26

---

## 1. Realization of Chinese Word Segmentation based on Deep Learning Method

1. **Long Short-Term Memory** : However, LSTM and RNN have the same problem, both of them from left to right to spread, result in the words in `the back are more important than the previous words` (**WHY?**), but for the word segmentation task, the weight of each word in the sentence should be `equal`. Thus, there is a `bidirectional LSTM` that combines the results of the LSTM which from left to right with the one that from right to left.
2. Word Embedding + Bidirectional LSTM +CRF Model<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/98df55f64253cd6b5f4aae79ed86f71.png)<br>
import the word embedding feature to the bidirectional LSTM, `add a linear layer to the hidden layer of the output` (***WHERE?***), and then add a CRF is to get the model implemented in this paper.
3. **EXPERIMENTAL RESULTS**: 
* **Chinese corpus resource**: the backoff2005 corpus provided by SIGHAN
* use the `test script of backoff2005` to test
* ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/5f888ab245b873ad93985dd0a68c438.png)
4. 使用了Word Embedding + Bidirectional LSTM +CRF Model结构进行分词

---------
## 2. A Deep Convolutional Neural Model for Character-Based Chinese Word Segmentation

1. ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/168fba429361fb7183c1238330652cb.png)
2. **position embedding**: a vector representation for each `position` in input sentence,which is expected to encode the information that are directly related to the `position`.<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/4c96c3815b9acb4124ec9ca3aecfbb8.png)<br>
    * **Unigram Embeddings**
    * **Bigram Embeddings**:the character
bigram embeddings are `pretrained` on unsupervised corpus<br>
    * If position j is in the middle of a multi-character word, it is likely that cj􀀀1cj and cjcj+1 share similar contexts`
in the unsupervised corpus, and thus have similar embeddings to some degree.
    * If position j is at the begin (or end) of a multi-character word, it is unlikely
that cj􀀀1cj and cjcj+1 share similar contexts and thus their embeddings are
dissimilar to each other.
3. **Deep Representation Module**
4. **Tag Scoring Module**:The tag scoring module transforms each position representation from a F-dimensional vector x(L)j into a K-dimension vector of tag scores yj . The tag set used is 'B', 'M', 'E', 'S',and hence K = 4
5. **Dropout**
6. **Tag Prediction and Word Segmentation**:After all positions have their tags predicted, the sentence is segmented in a simple heuristic way: A character with tag 'B' or 'S' will start a new word, while a character with tag 'M' or 'E' will append itself to the previous word.
7. **Model Training**
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/265f24f7be8bfcdef4c78250a2f16b4.png)<br>
Here, as a rule of thumb, we do not include a L2-regularization term in the loss function because both dropout and batch normalization have been used to regularize our model.
8. **Multi-task Learning**
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/a8db3d980ccfec8e872fff838c919b7.png)<br>
9. **Experiments**
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/a0197b1174e4a94d67d5fb6b9a20746.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/07e3dbea632f58b89ffcfdc255d3d21.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/9d602062fae46c22eedd4491696c991.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/839488cb4e9df0808c23cc4eba33ba4.png)<br>
10. However, the sequential processing mechanism of recurrent neural networks makes it `too costly to build hierarchical representations`. On the contrary, the deep convolutional network is suitable for the exploitation of modern multi-core computation ability such as GPU.
11. it is assumed that the induced position representations have `naturally contains` information about the tags for positions.
12. 使用position embeddings，unigram + bigram features，深度卷积网络，多任务学习

----
## 3. Glyph-aware Embedding of Chinese Characters

1. **Introduction**: At the same time, it is not correct to treat Chinese characters as equivalent to English words because the distribution of Chinese characters deviate markedly from Zipf’s law
2. **Hypotheses**: However, this hypothesis is not trivial because there are many Chinese characters that share strikingly similar visual appearances yet not their meanings.
3. **Method**: 
    * we decided to feed the glyph as an input to a feed-forward neural network (FNN) model, an embedder
    * we shall keep the recurrent neural network (RNN) architecture fixed and only change the embedder in our experiments.
    * we used CNN to implement the embedder<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/531de0e906c36513d67bbe0d4a8f428.png)<br>
[Google noto fonts](https://www.google.com/get/noto/help/cjk/)
    * we first render the glyph for a character using a font file and then feed the glyph as a gray-scale image into the CNN embedder.
    * [open-source : chinese-char-lm](https://github.com/falcondai/chinese-char-lm)

4. **Results**
    * Chinese language modeling<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/b207946f737a630b637adcbdad01060.png)<br>
    > **语言模型**（Language Model，LM），给出一句话的前k个词，希望它可以预测第k+1个词是什么，即给出一个第k+1个词可能出现的概率的分布p(xk+1|x1,x2,...,xk)。<br>
![](https://img-blog.csdn.net/20171224134258243)<br>
Perplexity可以认为是average branch factor（平均分支系数），即预测下一个词时可以有多少种选择。别人在作报告时说模型的PPL下降到90，可以直观地理解为，在模型生成一句话时下一个词有90个合理选择，可选词数越少，我们大致认为模型越准确。这样也能解释，为什么PPL越小，模型越好。<br>

        It seems that CNN embedder did not provide extra information useful for the task.<br>
        
    * Chinese word segmentation<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/c46927cc4bc250412a8e7e646c4e5fa.png)<br>
    > **GRU:** gated recurrent unit(Chung et al., 2014)<br>
    
    Overall, on both PKU and MSR, `the proposed mixed embedder` and `bidirectional LSTM` achieved the best performance outperforming the previous state-of-the-art on by a significant margin.
    
5. **Analysis**
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/3a062665954406715b5e0957f26ee24.png)<br>
**Need to be understand**

6. **Discussion**
the CNN embedder is a more compact representation with competitive performance as the ID embedder. 
7. **Conclusion**
glyph-aware embedding can improve performance in some Chinese NLP tasks, in particular, the word segmentation task.

----
## 4. Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components

1. **Introduction**
    * CBOW and Skip-Gram models are very popular due to their `simplicity` and `efficiency`
    * The components of characters can be roughly divided into two types: `semantic` component and `phonetic` component.
    * C&W model, [word2vec models](https://code.google.com/p/word2vec/), [CWE model](https://github.com/Leonard-Xu/CWE), multi-granularity embedding (MGE) model
    * Shi et al. (2015) proposed using WUBI input method to decompose the Chinese characters into components. However, WUBI input method uses rules to group Chinese characters into meaningless clusters which can fit the alphabet based keyboard. The semantics of the components are not straightforwardly meaningful.
    * [open-source : JWE](https://github.com/HKUST-KnowComp/JWE)

2. **Joint Learning Word Embedding**
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20180927093640.png)<br>
* the gradients of the embeddings of words, characters, subcharacter components can be different in our model while they are same in MGE.

3. **Experiments**
    * Training Corpus : [Chinese Wikipedia Dump](http://download.wikipedia.com/zhwiki)
    * Subcharacter Components : [HTTPCN](http://tool.httpcn.com/zi/)
    * Parameter Settings : 
    We fixed the `word vector dimension` to be 200, the `window size` to be 5, the `training iteration` to be 100, the `initial learning rate` to be 0.025, and the `subsampling parameter` to be 10^-􀀀4. Words with frequency less than 5 were `ignored` during training. We used 10-word `negative sampling` for optimization.

    * Word Similarity<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/8330d971be0acb014bc5831a8307849.png)<br>
    (***Q:What are components feature and radicals feature? What's the differences?***)<br>
        * From the results, we can see that JWE substantially outperforms CBOW, CWE, and MGE on the two word similarity datasets. JWE can better leverage the rich morphological information in Chinese words than CWE and MGE
        * It shows the benefits of `decoupling` the representation
        * characters are enough to provide additional semantic information for computing the similarities of many word pairs in the two datasets.
    
    * Word Analogy<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/5f4d9822a3f27ad34a99e12e8139b10.png)<br>
        * JWE outperforms the baselines on all categories’ word analogy tasks.
        * JWE with components consistently
performs better than JWE with radicals and JWE without either radicals or components. It demonstrates the necessary of delving deeper into finegrained components for complex semantic reasoning tasks.
    * Case Studies<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/f0896e85b8df21bbc1e12fef040b465.png)<br>
    It shows that JWE `does not overuse` the component information but `leverages both` the external context co-occurrence information and internal sub word morphological information well.
    
-----
## 5. cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information

-----
## 6. Joint Learning of Character andWord Embeddings
[open-source : CWE](https://github.com/Leonard-Xu/CWE)<br>

1. **Introduction**
    * Compared with words, Chinese characters are much more `ambiguous`. A character may play different roles and have various semantic meanings in different words. It will be insufficient to represent one character with only one vector.
    * Not all Chinese words are semantically compositional, such as `transliterated words`. The consideration of characters in these words will undermine the quality of em beddings for both words and characters

2. **Our Model**
    * We will take CBOW for example and demonstrate the framework of CWE based on CBOW.
    * Character-EnhancedWord Embedding
        * we find the concatenation operation, although being more time consuming, does not outperform the addition operation significantly, hence we only consider the addition operation for simplicity in this paper.<br>
        * ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/71df26e40e7298e7b7d08a3d618c539.png)<br>
        * 1/2 is crucial because it maintains similar length between embeddings of compositional and `noncompositional words`.<br>
    * Multiple-Prototype Character Embeddings
        * Position-based Character Embeddings<br>
        ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/3457b5c5aa3c225999d5be776f30ea5.png)<br>
        * Cluster-based Character Embeddings<br>
        ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/ec88cdb5453718fe33eb663b2195ac3.png)<br>
        (***Q:How to determine the vaule of Nc (clusters)***)<br>
        * Nonparametric Cluster-based Character Embeddings
     Following the idea of online nonparametric clustering algorithm, the number of clusters for a character is unknown, and is learned during training.<br>
        ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/aa190e93e2da323f9ef0e59b13f69ec.png)<br>
        (***What's the meaning? Does that mean add a clusters?***)

    * Word Selection for Learning
        * single-morpheme multi-character words (almost do not influence modeling)
        * transliterated words (build a word list) (***How to deal with them in detail?***)
        * entity names (perform Chinese POS tagging to identify)(***And then?***)
        
    * Initialization and Optimization
        Initialization with pre-trained character embeddings may achieve a slightly
better result.

3. **Experiments and Analysis**
    * Word Relatedness Computation<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/c80443c057feb2b5d8936b9155db0ce.png)<br>
        * CWE and its extensions all significantly outperform baseline methods
        * modeling multiple senses of characters is important for character embeddings
        * position information is not adequate in addressing ambiguity.
        * the baseline methods cannot handle these new words appropriately.
        * CWE methods will tend to misjudge the relatedness of two words with common characters.
        * 在相似度评估上，CWE对未登录词处理比词向量的方法效果较好，新词经常出现，而新字则出现很少，字向量包含的信息能有效处理未登录词，但是这样所带来的缺点是会让带有同一字的词距离更近，但带有同一字的词并不一定有相近的意思，论文中所举例子为：“肥皂剧”和“歌剧”，“电话”和“回话”（更细粒度的偏旁部首可能也会有同样的问题）

    * Analogical Reasoning<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/172aee9e1233cd75ec30e04cdf7c2fe.png)<br>
     
