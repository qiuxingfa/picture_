# 2018.10.10

---

* 使用论文 **Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components** 中的方法对word2vec产生的词向量进行评估
* 原文使用的是Chinese Wikipedia Dump进行训练，我使用的是自己在网上爬取的 @人民网，@人民日报 等十个新闻媒体账号近一百万条微博进行训练，使用和原文一样的训练参数<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/22eae216b2aeb09c8e0afb737ebd815.png)<br>
* Word Similarity <br>
    We select two different Chinese word similarity datasets, wordsim-240 and wordsim-296 provided by (Chen et al., 2015) for evaluation<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/8c0507aa8b094ba5227371fe1bf9e89.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/ca7491ef0770b7d9ad7ee3a910e313b.png)<br>

    |   CBOW    |   0.5272  |   0.5786  | 

    CWE : Chinese word embeddings<br>
    MGE : multi-granularity embedding<br>
    JWE: joint learning word embedding<br>
    +c represents the components feature<br>     +r represents the radicals feature<br> 
    +p indicates which subcharacters are used to predict the target word<br>
* Word Analogy<br>
测试文本里共有3种词对Capital，State，Family<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/b2cc582d58c7359a015bfc761490c3f.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/6a63291274d2989b6ea1c0be625cc26.png)<br>

    |   CBOW    |   0.7811  |   0.7947  |   0.9657  |   0.6287  |
    
    其中‘state’一栏效果很好，可能原因是语料中有很大一部分国内新闻，经常会提及国内地名

* 思考：
    * 测试文件的数据是否偏少，是否具有代表性，测试方法是否具有可信度？
    * 一个向量能不能代表一个词，有没有必要使用多向量表示？

* 下一步
    * skip-gram模型还在训练中
    * 搞懂测试的具体原理
    * 看书看论文






