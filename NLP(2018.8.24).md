# NLP(2018.8.24)

---

## 1. nlp-形式语言与自动机-ch07-自动分词、命名实体识别与词性标注
* `交集型切分歧义`：汉字串AJB称作交集型切分歧义，如果满足AJ、JB同时为词（A、J、B分别为汉字串）。 此时汉字串J称作交集串。（放风筝）
* `组合型歧义字段`：在字段ABC中，A、B、AB分别都是词表中的词（国内外贸易）
* `链长`：一个交集型切分歧义所拥有的交集串的集合称为交集串链，它的个数称为链长。
### 命名实体识别方法
![](https://img-blog.csdn.net/20170419101329398?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaF9qbHdnNjY4OA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

* 基于`CRF`的命名实体识别与前面介绍的基于字的汉语分词方法的原理一样，就是把命名实体识别过程看作一个序列标注问题。<br> 其`基本思路`是（以汉语为例）：将给定的文本首先进行分词处理，然后对人名、简单地名和简单的组织机构名进行识别，最后识别复合地名和复合组织机构名。 
* `特征模板`一般采用当前位置的前后n（n≥1）个位置上的字（或词、字母、数字、标点等，不妨统称为“字串”）及其标记表示，即以当前位置的前后n个位置范围内的字串及其标记作为观察窗口。考虑到，如果窗口开得较大时，算法的执行效率会太低，而且模板的通用性较差，但窗口太小时，所涵盖的信息量又太少，不足以确定当前位置上字串的标记，因此，一般情况下将n值取为2～3，即以当前位置上前后2～3个位置上的字串及其标记作为构成特征模型的符号。
* `在分词和词性标注的基础上进行命名实体识别的过程`就是对部分词语进行拆分、组合（确定实体边界）和重新分类（确定实体类别）的过程，最后输出一个最优的“词形／词性”序列。
### 1.1 汉语文本中交集型切分歧义的分类处理
* `汉语分词`：自动识别词边界，将汉字串切分为正确的词串
* 在汉语分词中对命名实体词汇的识别处理是指将命名实体中可独立成词的切分单位正确地识别出来，而不是指识别整个实体的左右边界
* `切分有向无环图`
* 关键问题
    * 通用词表和切分规范
    * 歧义字段的切分
    * 未登录词的识别
* 自动分词的方法
    * 机械匹配法
    * 特征词库法（相邻词）
    * 语法分析法
* 切分最大交集字段的几个原则
    * 尽量成词
    * 成语、熟语优先
    * 符合语法规则
    * 正向最大匹配

## 2. 统计自然语言处理梳理一：分词、命名实体识别、词性标注
* `词性标注`是在给定句子中判定每个词的语法范畴，确定其词性并加以标注的过程

## 3. 命名实体识别以及词性自动标注
* 命名实体的放射性
>举个栗子：“中国积极参与亚太经合组织的活动”，这里面的“亚太经合组织”是一个命名实体，定睛一瞧，这个实体着实不凡啊，有“组织”两个字，这么说来这个实体是一种组织或机构，记住，下一次当你看到“组织”的时候和前面几个字组成的一定是一个命名实体。继续观察，在它之前辐射出了“参与”一次，经过大规模语料训练后能发现，才“参与”后面有较大概率跟着一个命名实体。继续观察，在它之后有“的活动”，那么说明前面很可能是一个组织者，组织者多半是一个命名实体。这就是基于条件随机场做命名实体识别的奥秘，这就是命名实体的放射性

* `词类标注`特征工程和`中文分词`不同，在分词时最小元素是字，而词类标注虽然是针对词语序列做标签，可是每个词语的组成成分可能对标签有所影响，特别是在训练语料中缺失某词语时更需要参考词语的组成元素，例如首字，尾字，大小写等。<br>
在`BI-LSTM-CRF模型`中，可将每个词语的字向量输入`LSTM`(每个词语单独的LSTM网络)然后把其隐层和词向量连接。再接`双向lstm`->`投影层`->`crf`进行序列标注。

## 4. 中英命名实体识别及对齐中的中文分词优化
* `双语命名实体`在没有省略翻译、或者使用简称的情况下，往往边界是统一的．因此利用双语词对齐信息可以修正命名实体识别的错误<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.8.30/64b5153d8e70fa715380e8dabba4334.png)



* 然而，`中文命名实体识别`往往是在`分词`之后进行的，而分词系统往往在命名实体词汇上出现`分词错误`，特别是音译词汇。`分词错误`会直接造成命名实体的部分识别或者未识别。<br>
虽然现有的中文分词系统有较高的水平（Ｆ-score可以达到95％以上），但是在命名实体词汇的切分中常常出现错误，这主要是因为`命名实体词汇`往往是`未登录词`（OOV），而OOV造成的分词`精度失落`至少比分词歧义大５倍以上。<br>
此外，中文分词并没有统一的标准，现有的分词系统基本上是采用“结合紧密，使用稳定”作为分词粒度的界定标准，而实际上不同的应用，对于`分词粒度`有着不同的需求。<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.8.30/85a5a6c6654f830447b77b37a99d0d9.png)
* 采用了４个特征：`音译特征`、`意译特征`、`字对齐特征`、`同现特征`

## 5. 
* 加入CRF层是为了利用标签之间的局部依赖关系
* 加入 character-level 是为了缓解 OOV问题

## 6.序列标注与中文命名实体识别（NER）
* `BIEO标注方式`，即Begin, Intermediate, End, Other，针对不同的标注任务标注方式也各不相同
* 在上面的中文命名实体识别的例子中，我们感兴趣的是文本序列中的`实体`。在其他任务中会针对任务的不同打上不同的标签。
* 模型结构
    * `输入层`是一个将文本序列中的每个汉字利用预先训练好的字向量进行向量化，作为Bi-LSTM层的输入。<br>
    * 之后利用一个双向的LSTM(Bi-LSTM)对输入序列进行encode操作，也就是进行特征提取操纵。`采用双向LSTM`的效果要比单向的LSTM效果好，因为双向LSTM将序列正向和逆向均进行了遍历，相较于单向LSTM可以提取到更多的特征。<br>
    * 在经过双向LSTM层之后，我们这里使用一个`CRF层`进行decode，将Bi-LSTM层提取到的特征作为输入，然后利用CRF从这些特征中计算出序列中每一个元素的标签。

## 7.基于统计的机器学习的中文命名实体识别
* 其识别的好坏将直接影响分词精度以及其后的词性标洼和句法分板的精度，命名实体的自动识别也是汉语分词的关键问题和热点问题。

## 8
* ``词性标注``：给定一个词的序列（也就是句子），找出最可能的词性序列（标签是词性）。如ansj分词和ICTCLAS分词等。

* ``分词``：给定一个字的序列，找出最可能的标签序列（断句符号：[词尾]或[非词尾]构成的序列）。结巴分词目前就是利用BMES标签来分词的，B（开头）,M（中间),E(结尾),S(独立成词）

* ``命名实体识别``：给定一个词的序列，找出最可能的标签序列（内外符号：[内]表示词属于命名实体，[外]表示不属于）。如ICTCLAS实现的人名识别、翻译人名识别、地名识别都是用同一个Tagger实现的。

## 9
>首先任何字的标签不仅取决于它`自己`的参数，还取决于`前一个字`的标签。<br>
>但是`第一个字`前面并没有字，何来标签？所以第一个字的处理稍有不同，假设第0个字的标签为X，遍历X计算第一个字的标签，取分数最大的那一个。<br>
>如何计算一个字的某个标签的分数呢？某个字根据CRF模型提供的模板生成了一系列特征函数，这些函数的输出值乘以该函数的权值最后求和得出了一个分数。该分数只是“`点函数`”的得分，还需加上“`边函数`”的得分。边函数在本分词模型中简化为f(s',s)，其中s'为前一个字的标签，s为当前字的标签。于是该边函数就可以用一个4*4的矩阵描述，相当于HMM中的`转移概率`。实现了评分函数后，从第二字开始即可运用维特比后向解码，为所有字打上BEMS标签。
## CRF
* 统计概率图<br>
![](https://pic3.zhimg.com/v2-714c1843f78b6aecdb0c57cdd08e1c6a_r.jpg)
>`input`: "学习出一个模型，然后再预测出一条指定"<br>
>`expected output`: 学/B 习/E 出/S 一/B 个/E 模/B 型/E ，/S 然/B 后/E 再/E 预/B 测/E ……<br>
>`real output`: 学/B 习/E 出/S 一/B 个/B 模/B 型/E ，/S 然/B 后/B 再/E 预/B 测/E ……

* >用LSTM，整体的预测accuracy是不错indeed, 但是会出现上述的错误：在B之后再来一个B。这个错误在CRF中是不存在的，因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。当然了，CRF还能学到更多的限定特征，那越多越好啊！


## LSTM&CRF
* LSTM
>像RNN、LSTM、BILSTM这些模型，它们在`序列建模`上很强大，它们能够`capture长远的上下文信息`，此外还具备神经网络`拟合非线性的能力`，这些都是`crf无法超越`的地方
>对于t时刻来说，输出层y_t受到隐层h_t（包含上下文信息）和输入层x_t（当前的输入）的影响，但是`y_t`和其他时刻的`y_t'`是`相互独立`的，感觉像是一种point wise，对当前t时刻来说，我们希望找到一个概率最大的y_t，但`其他时刻的y_t'对当前y_t没有影响`，如果y_t之间存在较强的依赖关系的话（例如，形容词后面一般接名词，存在一定的约束），LSTM无法对这些约束进行建模，LSTM模型的性能将受到限制。

* CRF<br>
![](https://pic1.zhimg.com/v2-5223b0cb81c778e4abea09716ecbd306_r.jpg)
> 它不像LSTM等模型，能够考虑长远的上下文信息，它更多考虑的是整个句子的局部特征的线性加权组合（通过特征模版去扫描整个句子）。关键的一点是，CRF的模型为p(y | x, w)，注意这里y和x都是序列，它有点像list wise，优化的是一个序列y = (y1, y2, …, yn)，而不是某个时刻的y_t，即找到一个概率最高的序列y = (y1, y2, …, yn)使得p(y1, y2, …, yn| x, w)最高，它计算的是一种`联合概率`，优化的是`整个序列`（最终目标），而`不是将每个时刻的最优拼接起来`，在这一点上CRF要优于LSTM。

* LSTM+CRF
>把CRF接到LSTM上面，把LSTM在time_step上把每一个hidden_state的tensor输入给CRF，让LSTM负责在CRF的特征限定下，依照新的loss function，学习出一套新的非线性变换空间。<br>
>lstm加softmax分类的时候只能把`特征`的上下文关系学出来，label的没学出来。条件随机场可以把`label`的上下文学出来

## CRF Layer on the Top of BiLSTM
[通俗理解BiLSTM-CRF命名实体识别模型中的CRF层](https://www.cnblogs.com/createMoMo/p/7529885.html)

[github代码](https://github.com/createmomo/CRF-Layer-on-the-Top-of-BiLSTM)<br>

![](https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/CRF-LAYER-2-v2.png)

* 上图说明了BiLSTM层的`输出`是每个`标签`的`分数`。例如，对于w0，BiLSTM节点的输出为1.5（B-Person），0.9（I-Person），0.1（B-Organization），0.08（I-Organization）和0.05（O）。这些分数将是`CRF层的输入`。
* 在CRF层中，将选择具有`最高预测分数`的标签序列作为最佳答案。
<br>
1.  What if we DO NOT have the CRF layer<br>

![](https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/CRF-LAYER-3.jpg)<br>
![](https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/CRF-LAYER-4.jpg)<br>
显然，这次输出无效，“I-Organization I-Person”和“B-Organization I-Person”。
2. CRF layer can learn constrains from training data<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.8.30/043f8d915263775199b02b4f1df6c11.png)<br>

3. <br>
   ![](https://github.com/qiuxingfa/picture_/blob/master/2018.8.30/df31217338bbfef1d0c67c82cf47e9f.png)<br>
        
4. The total score of all the paths<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.8.30/7750a22ca813be7ec72da3c7caa67f9.png)<br>
    使用梯度下降等优化方法来求解参数。在这个过程中，我们要最大化真实标记序列的概率，也就训练了转移概率矩阵A和BiLSTM中的参数。<br>
   [totalscore具体推导过程](https://createmomo.github.io/2017/11/11/CRF-Layer-on-the-Top-of-BiLSTM-5/)

5. Infer the labels for a new sentence
    预测的时候，根据训练好的参数求出所有可能的y序列对应的s得分（这里应该也可以利用维特比算法）,然后取：<br>
![](https://pic1.zhimg.com/80/v2-0e8a1d97103a8c58d1a2d6e9802d0ecb_hd.jpg)<br>
做为预测结果输出。

## Character-Based LSTM-CRF with Radical-Level Features for Chinese Named Entity Recognition

1. `Character-based` tagging strategy achieves comparable performance `without results of Chinese Word Segmentation` (CWS), which means Chinese character can be the minimum unit to identify NEs
2. Recurrent Neural Network (`RNN`) learns `long distance dependencies` better than `CRF` which utilizes features found in a certain context window
3. Long Short-term Memory Networks (`LSTMs`) incorporate a memory-cell to combat this issue and have shown great capabilities to capture `long-range dependencies`.
4. The `context vector` of a character is obtained by concatenating its `left` and `right` context representations
5. [新华字典在线](http://tool.httpcn.com/Zi/)<br>

![](https://github.com/qiuxingfa/picture_/blob/master/2018.8.30/0cb0211982bcd371d273a1e039f15a0.png)<br>
6. Network Training<br>
   * We use Chinese Wikipedia backup dump of 20151201<br>
   * Character embeddings are pretrained using `CBOW` model because it’s faster than skipgram model.<br>
   * Radical embeddings are randomly initialized with dimension of 50<br>
   * We use stochastic gradient decent (`SGD`) algorithm with a learning rate of 0.05 for 50 epochs on training set.<br>
   
   
7. Experiments<br>
   * We test our model on MSRA data set of the third SIGHAN Bakeoff Chinese named entity recognition task.<br>
   * ![](https://github.com/qiuxingfa/picture_/blob/master/2018.8.30/e0ef7a6d6b7f389cc7c982a3fd369dc.png)<br>
   * We find that `radical-level LSTM` gives us an improvement of +0.53 in F1 with random initialized character embeddings. It is evident that radical-level information is `effective` for Chinese<br>
   * In other words, there are few characters initialized with random embeddings.So we `do not find further improvement` using both `radical-level LSTM` and `well pretrained character embeddings`<br>
   * Radical-level LSTM is obviously `effective` when there is no large corpus for character pretrainings.<br>
   * Our `BLSTM-CRF with radical embeddings` outperforms previous `best CRF model` by +3.27 in overall.<br>

## End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF
* We first use convolutional neural networks (`CNNs`) to encode character-level information of a word into its character-level representation. <br>
* Then we `combine` character- and word-level representations and feed them into bi-directional LSTM (`BLSTM`) to model context information of each word. <br>
* On top of BLSTM, we use a sequential `CRF` to jointly decode labels for the whole sentence.<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.8.30/df2ebf34fd3bf7b032ca9698c475148.png)<br>
1. CNN for Character-level Representation<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.8.30/b8c6ab7c3ea67d76b79aef11be5c6c5.png)<br>

2. Bi-directional LSTM<br>
The basic idea is to present each sequence `forwards` and `backwards` to two separate hidden states to capture `past` and `future` information, respectively<br>

3. CRF<br>



        




        
        
        






